{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3sQoe2Tg_fb"
      },
      "source": [
        "# 📚 Agentic RAG System with ArXiv + Web Fallback\n",
        "\n",
        "This project implements an **intelligent research assistant** that retrieves and synthesizes information using:\n",
        "1. **ArXiv papers** as the primary knowledge source (**RAG approach**)\n",
        "2. **Web search (Tavily API)** as a fallback mechanism\n",
        "3. **LangGraph** for orchestrating the decision-making workflow\n",
        "\n",
        "## 🎯 Purpose\n",
        "\n",
        "The system is designed to provide high-quality, research-backed answers to technical and scientific questions by:\n",
        "- Prioritizing academic and research papers from ArXiv for scientific queries\n",
        "- Falling back to web search for recent developments or non-academic topics\n",
        "- Maintaining conversation context for coherent multi-turn interactions\n",
        "- Ensuring proper attribution and citations in responses\n",
        "\n",
        "## 🔑 Prerequisites\n",
        "\n",
        "To use this system, you'll need:\n",
        "\n",
        "1. **OpenAI API Key**\n",
        "   - Required for:\n",
        "     - Text embeddings (for semantic search)\n",
        "     - Response generation (GPT-4 Turbo)\n",
        "     - Routing decisions (GPT-3.5 Turbo)\n",
        "   - Get it from: [OpenAI Platform](https://platform.openai.com)\n",
        "\n",
        "2. **Tavily API Key**\n",
        "   - Required for:\n",
        "     - Web search fallback functionality\n",
        "     - Real-time information retrieval\n",
        "     - Academic domain filtering\n",
        "   - Get it from: [Tavily](https://app.tavily.com)\n",
        "\n",
        "3. **Python Environment**\n",
        "   - Python 3.8 or higher\n",
        "   - Required packages (will be installed automatically):\n",
        "     - langchain-community\n",
        "     - langchain_chroma\n",
        "     - langchain_core\n",
        "     - langchain_openai\n",
        "     - langchain_text_splitters\n",
        "     - langgraph\n",
        "     - tavily-python\n",
        "     - openai\n",
        "     - python-dotenv\n",
        "\n",
        "\n",
        "## 🤖 Agentic Workflow Architecture\n",
        "\n",
        "The user workflow is translated into an agentic system through the following components:\n",
        "\n",
        "1. **State Management**\n",
        "   - **Conversation State**: Tracks user queries, system responses, and context\n",
        "   - **Search State**: Maintains information about current search results and sources\n",
        "   - **Decision State**: Stores routing decisions and their rationale\n",
        "\n",
        "2. **Agent Components**\n",
        "   - **Router Agent**: Makes intelligent decisions about information sources\n",
        "     - Analyzes query type and context\n",
        "     - Determines optimal search strategy\n",
        "     - Handles fallback mechanisms\n",
        "   \n",
        "   - **Search Agent**: Executes information retrieval\n",
        "     - Manages ArXiv API interactions\n",
        "     - Handles Tavily web search\n",
        "     - Processes and filters results\n",
        "   \n",
        "   - **Synthesis Agent**: Combines and formats information\n",
        "     - Merges multiple sources\n",
        "     - Ensures proper attribution\n",
        "     - Generates coherent responses\n",
        "\n",
        "3. **Feedback Loop**\n",
        "   - System learns from user interactions\n",
        "   - Improves routing decisions over time\n",
        "   - Adapts to user preferences and query patterns\n",
        "\n",
        "## 📊 Data Requirements and Sources\n",
        "\n",
        "The system requires and manages several types of data:\n",
        "\n",
        "1. **Input Data**\n",
        "   - **User Queries**: Natural language questions and follow-ups\n",
        "   - **Conversation History**: Previous interactions for context\n",
        "   - **User Preferences**: Optional settings for search behavior\n",
        "\n",
        "2. **Knowledge Sources**\n",
        "   - **ArXiv Papers**:\n",
        "     - Source: ArXiv API\n",
        "     - Format: PDF documents\n",
        "     - Update Frequency: Daily\n",
        "     - Coverage: Scientific and technical papers\n",
        "   \n",
        "   - **Web Content**:\n",
        "     - Source: Tavily API\n",
        "     - Format: Web pages and documents\n",
        "     - Update Frequency: Real-time\n",
        "     - Coverage: News, blogs, documentation, etc.\n",
        "\n",
        "3. **Processed Data**\n",
        "   - **Embeddings**: Vector representations of text\n",
        "     - Generated using OpenAI's embedding model\n",
        "     - Stored in vector database\n",
        "   \n",
        "   - **Chunks**: Processed text segments\n",
        "     - Size: Optimized for semantic search\n",
        "     - Metadata: Source, date, relevance score\n",
        "   \n",
        "   - **Citations**: Reference information\n",
        "     - Paper titles, authors, URLs\n",
        "     - Web page sources and dates\n",
        "\n",
        "4. **Output Data**\n",
        "   - **Responses**: Generated answers with citations\n",
        "   - **Search Results**: Ranked and filtered information\n",
        "   - **Conversation Logs**: Interaction history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0zyWACuhcfqG",
        "outputId": "b83c68f2-26c0-4f4d-a1bf-0bdd54388342"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-community 0.0.20 requires langchain-core<0.2,>=0.1.21, but you have langchain-core 0.3.51 which is incompatible.\n",
            "langchain-community 0.0.20 requires langsmith<0.1,>=0.0.83, but you have langsmith 0.3.30 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# %% [code]\n",
        "# Install required packages\n",
        "! pip install -qU langchain langgraph pypdf chromadb tavily-python openai python-dotenv pyboxen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ai9xIIrecwKb",
        "outputId": "0c889bf8-bda7-4b27-8fec-05e19c1c5aaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-community in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (0.0.20)\n",
            "Collecting langchain_chroma\n",
            "  Downloading langchain_chroma-0.2.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: langchain_core in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (0.3.51)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.12-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain_text_splitters in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (0.3.8)\n",
            "Requirement already satisfied: langgraph in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (0.3.29)\n",
            "Requirement already satisfied: tavily-python in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (0.5.4)\n",
            "Requirement already satisfied: openai in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (1.74.0)\n",
            "Requirement already satisfied: python-dotenv in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (1.1.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from langchain-community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from langchain-community) (3.10.10)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from langchain-community) (0.6.7)\n",
            "Collecting langchain_core\n",
            "  Using cached langchain_core-0.1.53-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langsmith<0.1,>=0.0.83 (from langchain-community)\n",
            "  Using cached langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from langchain-community) (8.5.0)\n",
            "INFO: pip is looking at multiple versions of langchain-chroma to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain_chroma\n",
            "  Downloading langchain_chroma-0.2.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0 (from langchain_chroma)\n",
            "  Downloading chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: fastapi<1,>=0.95.2 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from langchain_chroma) (0.115.9)\n",
            "Collecting langchain_chroma\n",
            "  Downloading langchain_chroma-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0 (from langchain_chroma)\n",
            "  Downloading chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting langchain_chroma\n",
            "  Downloading langchain_chroma-0.1.4-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from langchain_core) (1.33)\n",
            "INFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain_core\n",
            "  Using cached langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.51-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.50-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.49-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.48-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.47-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.46-py3-none-any.whl.metadata (5.9 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached langchain_core-0.1.45-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.44-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.43-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.42-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.41-py3-none-any.whl.metadata (5.9 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached langchain_core-0.1.40-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.39-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.38-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.37-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.36-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.35-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.34-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.33-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from langchain_core) (4.6.2)\n",
            "  Using cached langchain_core-0.1.32-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.31-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.30-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.29-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.28-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.27-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.26-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.25-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.24-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.23-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting langsmith<0.1,>=0.0.83 (from langchain-community)\n",
            "  Using cached langsmith-0.0.87-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from langchain_core) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from langchain_core) (2.9.2)\n",
            "Collecting langchain_chroma\n",
            "  Downloading langchain_chroma-0.1.3-py3-none-any.whl.metadata (1.5 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain-chroma to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading langchain_chroma-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading langchain_chroma-0.1.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading langchain_chroma-0.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting chromadb<0.5.0,>=0.4.0 (from langchain_chroma)\n",
            "  Downloading chromadb-0.4.24-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from langchain-community) (0.3.23)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from langchain-community) (0.3.30)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from langchain_core) (4.12.2)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from langchain_openai) (0.8.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from langgraph) (2.0.24)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from langgraph) (0.1.8)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from langgraph) (0.1.61)\n",
            "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: httpx in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from tavily-python) (0.27.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from openai) (4.67.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from anyio<5,>=3->langchain_core) (3.10)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from anyio<5,>=3->langchain_core) (1.2.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.2.2.post1)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.7.6)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.32.0)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.24.1)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.21.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.32.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.32.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.53b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.32.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.20.3)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (7.4.0)\n",
            "Requirement already satisfied: importlib-resources in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.71.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.13.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (32.0.1)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.10.11)\n",
            "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (13.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from httpx->tavily-python) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from httpx->tavily-python) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from httpcore==1.*->httpx->tavily-python) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.9.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from pydantic<3,>=1->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from pydantic<3,>=1->langchain_core) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: pyproject_hooks in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from build>=1.0.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from build>=1.0.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.0.1)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from fastapi<1,>=0.95.2->langchain_chroma) (0.41.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.2.2)\n",
            "Requirement already satisfied: durationpy>=0.7 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.9)\n",
            "Requirement already satisfied: coloredlogs in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (5.29.4)\n",
            "Requirement already satisfied: sympy in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.32.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.32.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.32.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.32.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.53b0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.53b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.53b0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.53b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.53b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.53b0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.53b0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.53b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.17.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.53b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.15.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.26.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.5.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.6.4)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.0.5)\n",
            "Requirement already satisfied: websockets>=10.4 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (12.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (4.9)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2024.10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.1.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /opt/anaconda3/envs/gradio_env/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.6.1)\n",
            "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_chroma-0.2.2-py3-none-any.whl (11 kB)\n",
            "Downloading langchain_openai-0.3.12-py3-none-any.whl (61 kB)\n",
            "Downloading chromadb-0.6.3-py3-none-any.whl (611 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Installing collected packages: httpx-sse, pydantic-settings, langchain_openai, langchain-community, chromadb, langchain_chroma\n",
            "  Attempting uninstall: langchain-community\n",
            "    Found existing installation: langchain-community 0.0.20\n",
            "    Uninstalling langchain-community-0.0.20:\n",
            "      Successfully uninstalled langchain-community-0.0.20\n",
            "  Attempting uninstall: chromadb\n",
            "    Found existing installation: chromadb 1.0.4\n",
            "    Uninstalling chromadb-1.0.4:\n",
            "      Successfully uninstalled chromadb-1.0.4\n",
            "Successfully installed chromadb-0.6.3 httpx-sse-0.4.0 langchain-community-0.3.21 langchain_chroma-0.2.2 langchain_openai-0.3.12 pydantic-settings-2.8.1\n"
          ]
        }
      ],
      "source": [
        "! pip install langchain-community langchain_chroma langchain_core langchain_openai langchain_text_splitters langgraph tavily-python openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MDJAvAUpcjmN"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "# Import required libraries\n",
        "import os  # Provides functions to interact with the operating system.\n",
        "from pyboxen import boxen  # Used to display stylized boxes in the terminal for better CLI UI.\n",
        "from getpass import getpass  # Allows secure password input without echoing.\n",
        "from typing import TypedDict, List, Dict, Optional, Literal, Union, Annotated, cast  # Used for type annotations and static type checking.\n",
        "from langchain_core.documents import Document  # Represents and structures text data in LangChain.\n",
        "from langchain_core.output_parsers import StrOutputParser  # Parses raw LLM output into usable string format.\n",
        "from langchain_community.document_loaders import PyPDFLoader  # Loads and extracts text from PDF documents.\n",
        "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter  # Splits text into chunks using markdown headers or character limits.\n",
        "from langchain_chroma import Chroma  # Provides integration with Chroma vector store for embedding storage and retrieval.\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI  # Interfaces with OpenAI for embeddings and chat models.\n",
        "from langchain_core.prompts import ChatPromptTemplate  # Manages prompt templates for chat-based interactions.\n",
        "from langgraph.graph import StateGraph, END  # Helps define state-based logic flows for chat systems.\n",
        "from tavily import TavilyClient  # Interfaces with Tavily for real-time web search.\n",
        "from langchain.memory import ConversationBufferMemory  # Maintains memory of past conversation for context retention.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOl5svrxoDkQ"
      },
      "source": [
        "# API Key Submission\n",
        "\n",
        "Please follow the instructions below:\n",
        "\n",
        "1. **Provide the Tavily API Key**\n",
        "2. **Provide the Open API Key**\n",
        "3. **Press Enter** to proceed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "y1CrD7yecpRF",
        "outputId": "f3c26c3f-454a-4909-8828-fe1680ee3b97"
      },
      "outputs": [],
      "source": [
        "# Set API keys\n",
        "os.environ[\"TAVILY_API_KEY\"] = getpass(\"Enter Tavily API Key (get from https://app.tavily.com): \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVsoCt4BhfRp"
      },
      "source": [
        "## 2. Define State and System Architecture\n",
        "\n",
        "We'll define our system's state and flow using **LangGraph**. The state will track our:\n",
        "- **Input question**\n",
        "- **Retrieved ArXiv results**\n",
        "- **Web search results**\n",
        "- **Final answer**\n",
        "- **Conversation history for context**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9DX0VBugddBN"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "# Define our system state - this is what passes between nodes in our graph\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"State definition for our agentic RAG system\"\"\"\n",
        "    question: str  # User's current question\n",
        "    arxiv_results: Optional[List[Document]]  # Results from ArXiv papers (if any)\n",
        "    web_results: Optional[List[Dict]]  # Results from web search (if any)\n",
        "    answer: str  # Final synthesized answer\n",
        "    conversation_history: str  # Previous Q&A for context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym7UTPc4hpli"
      },
      "source": [
        "## 3. Router Node Implementation\n",
        "\n",
        "The **Router Node** is responsible for deciding whether to use **ArXiv papers** or **web search**.\n",
        "- **First**, it tries to use **ArXiv papers** (our local knowledge source).\n",
        "- **Falls back** to **web search** if needed.\n",
        "\n",
        "This demonstrates **strategic decision-making capabilities**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4oSbkgqwdh_R"
      },
      "outputs": [],
      "source": [
        "router_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a highly specialized research assistant with access to two information sources:\n",
        "1. A collection of ArXiv research papers\n",
        "2. A web search tool\n",
        "\n",
        "Your task is to determine which source would be better to answer the user's question.\n",
        "FIRST try to use ArXiv papers for scientific and academic questions.\n",
        "ONLY use web search if:\n",
        "- The question requires very recent information not likely in research papers\n",
        "- The question is about general knowledge, news, or non-academic topics\n",
        "- The question asks for information beyond what academic papers would contain\n",
        "\n",
        "Consider the conversation history for context.\n",
        "\n",
        "Question: {question}\n",
        "Conversation History: {conversation_history}\n",
        "\n",
        "Respond with ONLY ONE of these two options:\n",
        "\"arxiv\" - if the question should be answered using research papers\n",
        "\"web\" - if the question requires web search\n",
        "\n",
        "Your decision should be a single word only (either \"arxiv\" or \"web\"). Do not include any explanation, reasoning, or additional text in your response.\n",
        "\"\"\")\n",
        "\n",
        "def router_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Determines whether to use ArXiv papers or web search based on the question.\n",
        "\n",
        "    Args:\n",
        "        state: Current state containing the question and conversation history\n",
        "\n",
        "    Returns:\n",
        "        Dict indicating which path to take next\n",
        "    \"\"\"\n",
        "    # Use a lighter model for routing decisions\n",
        "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "    # Create a chain that outputs just the decision text\n",
        "    chain = router_prompt | llm\n",
        "\n",
        "    # Invoke the chain with our question and history\n",
        "    # Get the content of the AIMessage object instead of directly calling strip()\n",
        "    decision = chain.invoke({\n",
        "        \"question\": state[\"question\"],\n",
        "        \"conversation_history\": state[\"conversation_history\"]\n",
        "    }).content.strip().lower()\n",
        "\n",
        "    print(f\"Router decision: {decision}\")\n",
        "\n",
        "    # Return the next node to be called based on the decision\n",
        "    if \"web\" in decision:\n",
        "        return {\"next\": \"web_search\"}\n",
        "    else:\n",
        "        return {\"next\": \"arxiv_retrieval\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D_WGWpKhz4D"
      },
      "source": [
        "# ArXiv Processor Documentation\n",
        "\n",
        "## Overview\n",
        "The `ArXivProcessor` class is designed to handle processing ArXiv PDFs for retrieval-augmented generation (RAG) systems. It implements document-aware chunking strategies specifically optimized for scientific papers.\n",
        "\n",
        "## Key Features\n",
        "- **Two-step chunking strategy**:\n",
        " 1. Markdown header splitting to preserve document structure\n",
        " 2. Recursive character splitting for handling longer sections effectively\n",
        "- **Confidence-based retrieval** with threshold filtering\n",
        "- **Metadata preservation** from original PDFs\n",
        "\n",
        "## Class Structure\n",
        "\n",
        "### Constructor: `__init__()`\n",
        "Initializes the processor with specialized document chunking strategies:\n",
        "- `MarkdownHeaderTextSplitter` to maintain document section structure\n",
        "- `RecursiveCharacterTextSplitter` for detailed content subdivision\n",
        "\n",
        "### Methods\n",
        "\n",
        "#### `load_and_process(pdf_urls: List[str])`\n",
        "Processes ArXiv PDFs with document-aware chunking:\n",
        "- Loads PDFs from provided URLs\n",
        "- Converts content to markdown-style text with headers\n",
        "- Applies two-stage chunking process\n",
        "- Creates a vector store with OpenAI embeddings\n",
        "\n",
        "#### `retrieve(question: str, confidence_threshold: float = 0.75, k: int = 5)`\n",
        "Retrieves relevant chunks with confidence scoring:\n",
        "- Performs similarity search based on user query\n",
        "- Filters results by confidence threshold\n",
        "- Returns only high-relevance document chunks\n",
        "\n",
        "## Implementation Example\n",
        "The documented code includes a sample implementation that loads and processes two ArXiv papers:\n",
        "- Quantum computing paper: https://arxiv.org/pdf/2305.10343.pdf\n",
        "- LLM research paper: https://arxiv.org/pdf/2303.04137.pdf\n",
        "\n",
        "## Dependencies\n",
        "- `PyPDFLoader` for PDF handling\n",
        "- `MarkdownHeaderTextSplitter` and `RecursiveCharacterTextSplitter` for content chunking\n",
        "- `OpenAIEmbeddings` for vector embeddings\n",
        "- `Chroma` for vector storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "QeTPjOKgdom7",
        "outputId": "2dcd9fba-1c9c-4563-eb4c-aed41d524a0b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m╭─\u001b[0m\u001b[36m >>> Initialization \u001b[0m\u001b[36m──────────────────────────────────\u001b[0m\u001b[36m─╮\u001b[0m                                                         \n",
            "\u001b[36m│\u001b[0m                                                        \u001b[36m│\u001b[0m                                                         \n",
            "\u001b[36m│\u001b[0m   Initializing ArXiv processor with sample papers...   \u001b[36m│\u001b[0m                                                         \n",
            "\u001b[36m│\u001b[0m                                                        \u001b[36m│\u001b[0m                                                         \n",
            "\u001b[36m╰────────────────────────────────────────────────────────╯\u001b[0m                                                         \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m╭─\u001b[0m\u001b[34m >>> PDF Loading \u001b[0m\u001b[34m────────────────────────────────────────\u001b[0m\u001b[34m─╮\u001b[0m                                                      \n",
            "\u001b[34m│\u001b[0m                                                           \u001b[34m│\u001b[0m                                                      \n",
            "\u001b[34m│\u001b[0m   Loading PDF from https://arxiv.org/pdf/2305.10343.pdf   \u001b[34m│\u001b[0m                                                      \n",
            "\u001b[34m│\u001b[0m                                                           \u001b[34m│\u001b[0m                                                      \n",
            "\u001b[34m╰───────────────────────────────────────────────────────────╯\u001b[0m                                                      \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m╭─\u001b[0m\u001b[34m >>> PDF Loading \u001b[0m\u001b[34m────────────────────────────────────────\u001b[0m\u001b[34m─╮\u001b[0m                                                      \n",
            "\u001b[34m│\u001b[0m                                                           \u001b[34m│\u001b[0m                                                      \n",
            "\u001b[34m│\u001b[0m   Loading PDF from https://arxiv.org/pdf/2303.04137.pdf   \u001b[34m│\u001b[0m                                                      \n",
            "\u001b[34m│\u001b[0m                                                           \u001b[34m│\u001b[0m                                                      \n",
            "\u001b[34m╰───────────────────────────────────────────────────────────╯\u001b[0m                                                      \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "could not convert string to float: b'0.00-28645087' : FloatObject (b'0.00-28645087') invalid; use 0.0 instead\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m╭─\u001b[0m\u001b[32m >>> Processing Complete \u001b[0m\u001b[32m─────────\u001b[0m\u001b[32m─╮\u001b[0m                                                                             \n",
            "\u001b[32m│\u001b[0m                                    \u001b[32m│\u001b[0m                                                                             \n",
            "\u001b[32m│\u001b[0m   Created 171 chunks from 2 PDFs   \u001b[32m│\u001b[0m                                                                             \n",
            "\u001b[32m│\u001b[0m                                    \u001b[32m│\u001b[0m                                                                             \n",
            "\u001b[32m╰────────────────────────────────────╯\u001b[0m                                                                             \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m╭─\u001b[0m\u001b[32m >>> Status \u001b[0m\u001b[32m────────────────────\u001b[0m\u001b[32m─╮\u001b[0m                                                                               \n",
            "\u001b[32m│\u001b[0m                                  \u001b[32m│\u001b[0m                                                                               \n",
            "\u001b[32m│\u001b[0m   ArXiv processor initialized!   \u001b[32m│\u001b[0m                                                                               \n",
            "\u001b[32m│\u001b[0m                                  \u001b[32m│\u001b[0m                                                                               \n",
            "\u001b[32m╰──────────────────────────────────╯\u001b[0m                                                                               \n",
            "\n"
          ]
        }
      ],
      "source": [
        "class ArXivProcessor:\n",
        "    \"\"\"\n",
        "    Handles processing ArXiv PDFs for retrieval-augmented generation.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the processor with document-aware chunking strategies.\n",
        "\n",
        "        The chunking strategy uses a two-step approach:\n",
        "        1. Markdown header splitting preserves document structure and headers\n",
        "        2. Recursive character splitting handles longer sections effectively\n",
        "        \"\"\"\n",
        "        # Header splitter preserves section structure in scientific papers\n",
        "        self.header_splitter = MarkdownHeaderTextSplitter(\n",
        "            headers_to_split_on=[\n",
        "                (\"#\", \"Section\"),           # Main sections\n",
        "                (\"##\", \"Subsection\"),       # Subsections\n",
        "                (\"###\", \"Subsubsection\")    # Sub-subsections\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Recursive splitter handles nested hierarchies and technical content\n",
        "        # - Chunk size of 1000 balances context vs specificity\n",
        "        # - Overlap of 200 ensures continuity between chunks\n",
        "        # - Separators prioritize natural breaks in scientific text\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
        "        )\n",
        "\n",
        "        # Will be initialized when documents are loaded\n",
        "        self.vector_store = None\n",
        "\n",
        "    def load_and_process(self, pdf_urls: List[str]):\n",
        "        \"\"\"\n",
        "        Process ArXiv PDFs with document-aware chunking\n",
        "\n",
        "        Args:\n",
        "            pdf_urls: List of URLs to ArXiv PDFs\n",
        "        \"\"\"\n",
        "        all_chunks = []\n",
        "\n",
        "        # Process each PDF\n",
        "        for url in pdf_urls:\n",
        "            print(boxen(f\"Loading PDF from {url}\", title=\">>> PDF Loading\", color=\"blue\", padding=1))\n",
        "            loader = PyPDFLoader(url)\n",
        "            pages = loader.load()\n",
        "\n",
        "            # Process each page\n",
        "            for page in pages:\n",
        "                # Convert PDF content to markdown-style text with headers\n",
        "                page_text = f\"# {page.metadata['source']}\\n## Page {page.metadata['page']}\\n{page.page_content}\"\n",
        "\n",
        "                # First split by headers to maintain document structure\n",
        "                header_chunks = self.header_splitter.split_text(page_text)\n",
        "\n",
        "                # Then split large sections into smaller chunks\n",
        "                small_chunks = self.text_splitter.split_documents(header_chunks)\n",
        "\n",
        "                # Add to our collection\n",
        "                all_chunks.extend(small_chunks)\n",
        "\n",
        "        print(boxen(f\"Created {len(all_chunks)} chunks from {len(pdf_urls)} PDFs\", title=\">>> Processing Complete\", color=\"green\", padding=1))\n",
        "\n",
        "        # Create vector store with OpenAI embeddings\n",
        "        self.vector_store = Chroma.from_documents(\n",
        "            documents=all_chunks,\n",
        "            embedding=OpenAIEmbeddings(),\n",
        "            persist_directory=\"./arxiv_db\"\n",
        "        )\n",
        "\n",
        "    def retrieve(self, question: str, confidence_threshold: float = 0.75, k: int = 5):\n",
        "        \"\"\"\n",
        "        Retrieve relevant chunks with confidence scoring\n",
        "\n",
        "        Args:\n",
        "            question: User question to find relevant information for\n",
        "            confidence_threshold: Minimum relevance score (0-1) to include a result\n",
        "            k: Maximum number of results to return\n",
        "\n",
        "        Returns:\n",
        "            List of relevant document chunks that meet the threshold\n",
        "        \"\"\"\n",
        "        if not self.vector_store:\n",
        "            raise ValueError(\"No ArXiv documents loaded. Run load_and_process first.\")\n",
        "\n",
        "        # Perform similarity search with relevance scores\n",
        "        results = self.vector_store.similarity_search_with_relevance_scores(\n",
        "            question, k=k\n",
        "        )\n",
        "\n",
        "        # Filter by confidence threshold\n",
        "        filtered_results = [doc for doc, score in results if score >= confidence_threshold]\n",
        "\n",
        "        print(boxen(f\"Found {len(filtered_results)} relevant chunks above threshold {confidence_threshold}\", title=\">>> Retrieval Results\", color=\"yellow\", padding=1))\n",
        "\n",
        "        return filtered_results\n",
        "\n",
        "# Load sample ArXiv PDFs\n",
        "print(boxen(\"Initializing ArXiv processor with sample papers...\", title=\">>> Initialization\", color=\"cyan\", padding=1))\n",
        "arxiv_processor = ArXivProcessor()\n",
        "arxiv_processor.load_and_process([\n",
        "    \"https://arxiv.org/pdf/2305.10343.pdf\",  # Quantum computing paper\n",
        "    \"https://arxiv.org/pdf/2303.04137.pdf\"   # LLM research paper\n",
        "])\n",
        "print(boxen(\"ArXiv processor initialized!\", title=\">>> Status\", color=\"green\", padding=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeaD77BupxMv"
      },
      "source": [
        "# ArXiv Retrieval Node Documentation\n",
        "\n",
        "## Overview\n",
        "The `arxiv_retrieval_node` function serves as a retrieval component in an agent-based system, fetching relevant scientific information from ArXiv papers based on user queries.\n",
        "\n",
        "## Function Signature\n",
        "`arxiv_retrieval_node(state: AgentState) -> dict`\n",
        "\n",
        "## Parameters\n",
        "- `state`: An AgentState object containing the current conversation state, including:\n",
        " - `question`: The user's query to search for in ArXiv papers\n",
        "\n",
        "## Functionality\n",
        "The function:\n",
        "1. Extracts the user's question from the input state\n",
        "2. Calls the `arxiv_processor.retrieve()` method to find relevant document chunks\n",
        "3. Uses a reduced confidence threshold (0.5) compared to the default (0.75) to improve recall\n",
        "4. Returns the retrieved documents for further processing\n",
        "\n",
        "## Return Value\n",
        "Returns a dictionary with:\n",
        "- `arxiv_results`: A list of document chunks from ArXiv papers relevant to the user's question\n",
        "\n",
        "## Integration Notes\n",
        "- This function is designed to be used as a node in an agent workflow\n",
        "- The reduced confidence threshold ensures more potential matches are returned, prioritizing recall over precision\n",
        "- The retrieved documents can be used by subsequent nodes for answering the user's question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7QI0RXSbdvCv"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "def arxiv_retrieval_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Retrieves relevant information from ArXiv papers based on the question.\n",
        "\n",
        "    Args:\n",
        "        state: Current state containing the question\n",
        "\n",
        "    Returns:\n",
        "        Updated state with arxiv_results\n",
        "    \"\"\"\n",
        "    # Retrieve relevant documents from ArXiv\n",
        "    relevant_docs = arxiv_processor.retrieve(\n",
        "        question=state[\"question\"],\n",
        "        confidence_threshold=0.5  # Adjusted threshold for better recall\n",
        "    )\n",
        "\n",
        "    # Check if we found enough relevant content\n",
        "    return {\"arxiv_results\": relevant_docs}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SqO1XREiD7z"
      },
      "source": [
        "## 5. Web Search Node Implementation\n",
        "\n",
        "The **Web Search Node** uses the **Tavily API** to search the web when **ArXiv papers** don't have the answer.\n",
        "\n",
        "- **Optimizes** the search query\n",
        "- **Filters and processes** results\n",
        "- **Ensures** proper attribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "15KqqPGedzln"
      },
      "outputs": [],
      "source": [
        "class WebSearcher:\n",
        "    \"\"\"\n",
        "    Handles web search functionality using the Tavily API.\n",
        "    \"\"\"\n",
        "    def __init__(self, api_key: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Initialize with Tavily API client.\n",
        "\n",
        "        Args:\n",
        "            api_key: Tavily API key (uses environment variable if None)\n",
        "        \"\"\"\n",
        "        if api_key:\n",
        "            self.client = TavilyClient(api_key=api_key)\n",
        "        else:\n",
        "            self.client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
        "\n",
        "    def search(self, query: str, max_results: int = 5, include_domains: Optional[List[str]] = None):\n",
        "        \"\"\"\n",
        "        Search the web with query optimization.\n",
        "\n",
        "        Args:\n",
        "            query: User question\n",
        "            max_results: Maximum number of results to return\n",
        "            include_domains: Optional list of domains to prioritize\n",
        "\n",
        "        Returns:\n",
        "            List of search results with content and URL\n",
        "        \"\"\"\n",
        "        # Optimize the query for academic research\n",
        "        # Biases towards academic and scientific sources\n",
        "        domain_filter = \"\"\n",
        "        if include_domains:\n",
        "            domain_filter = \" OR \".join([f\"site:{domain}\" for domain in include_domains])\n",
        "            domain_filter = f\"({domain_filter}) \"\n",
        "\n",
        "        optimized_query = f\"{domain_filter}{query}\"\n",
        "\n",
        "        print(boxen(f\"Searching with query: {optimized_query}\", title=\">>> Web Search Query\", color=\"blue\", padding=1))\n",
        "\n",
        "        # Execute search through Tavily\n",
        "        response = self.client.search(\n",
        "            query=optimized_query,\n",
        "            search_depth=\"advanced\",  # More comprehensive search\n",
        "            include_answer=True,      # Get an AI-generated answer too\n",
        "            max_results=max_results\n",
        "        )\n",
        "\n",
        "        # Extract relevant information from results\n",
        "        results = [\n",
        "            {\n",
        "                \"content\": r[\"content\"],\n",
        "                \"url\": r[\"url\"],\n",
        "                \"title\": r.get(\"title\", \"Unknown Title\")\n",
        "            }\n",
        "            for r in response[\"results\"]\n",
        "        ]\n",
        "\n",
        "        print(boxen(f\"Found {len(results)} web results\", title=\">>> Search Results\", color=\"yellow\", padding=1))\n",
        "\n",
        "        return results\n",
        "\n",
        "# Initialize web searcher\n",
        "web_searcher = WebSearcher()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_WsFmnmpd3CI"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "def web_search_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Searches the web for information using the Tavily API.\n",
        "\n",
        "    Args:\n",
        "        state: Current state containing the question\n",
        "\n",
        "    Returns:\n",
        "        Updated state with web_results\n",
        "    \"\"\"\n",
        "    # Include academic domains to improve search quality\n",
        "    academic_domains = [\"arxiv.org\", \"scholar.google.com\", \"researchgate.net\", \"edu\"]\n",
        "\n",
        "    # Get search results\n",
        "    results = web_searcher.search(\n",
        "        query=state[\"question\"],\n",
        "        max_results=5,\n",
        "        include_domains=academic_domains\n",
        "    )\n",
        "\n",
        "    return {\"web_results\": results}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYoo-tCciKHn"
      },
      "source": [
        "# Function Documentation: `synthesize_answer_node`\n",
        "\n",
        "## Overview\n",
        "The `synthesize_answer_node` function is a key component in a LangChain-based conversational agent. It is responsible for generating a comprehensive answer based on either scientific research papers (from ArXiv) or web search results (via Tavily). The generated response is contextual, well-structured, and strictly grounded in the retrieved data.\n",
        "\n",
        "---\n",
        "\n",
        "## Purpose\n",
        "To synthesize a high-quality, structured, and citation-backed answer from the information retrieved during the conversational flow — either from ArXiv research papers or real-time web search results.\n",
        "\n",
        "---\n",
        "\n",
        "## Inputs\n",
        "\n",
        "- **state (AgentState)**:  \n",
        "  A dictionary representing the current state of the agent, which includes:\n",
        "  - `question`: The user's query.\n",
        "  - `arxiv_results`: A list of research paper excerpts (if available).\n",
        "  - `web_results`: A list of web search results (used when no ArXiv data is present).\n",
        "  - `conversation_history`: Context from previous exchanges to maintain continuity.\n",
        "\n",
        "---\n",
        "\n",
        "## Logic Flow\n",
        "\n",
        "1. **Source Determination**:  \n",
        "   The function first checks whether ArXiv results are available. If so, it uses them; otherwise, it falls back to web search results.\n",
        "\n",
        "2. **Prompt Construction**:  \n",
        "   A custom `prompt_template` is built depending on the data source. Each template includes:\n",
        "   - The original question.\n",
        "   - Retrieved content (formatted accordingly).\n",
        "   - Prior conversation context.\n",
        "   - Explicit instructions to ensure grounded, factual, and well-structured responses.\n",
        "\n",
        "3. **Model Invocation**:  \n",
        "   - Uses `ChatOpenAI` (specifically `gpt-4-turbo`) for advanced reasoning and response generation.\n",
        "   - Combines the prompt and model into a LangChain chain using `ChatPromptTemplate` and `StrOutputParser`.\n",
        "\n",
        "4. **Response Handling**:  \n",
        "   - If web results were used, the function appends a list of source URLs at the end of the response.\n",
        "   - If ArXiv sources were used, inline citations in the format `(Author et al., Page X)` are expected.\n",
        "\n",
        "---\n",
        "\n",
        "## Output\n",
        "\n",
        "- Returns a dictionary with a single key:  \n",
        "  - `answer`: A fully formatted, cited response derived from either research papers or search results.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Characteristics\n",
        "\n",
        "- **Grounded Output**: The model is instructed not to hallucinate or invent facts.\n",
        "- **Citations Included**: Adds credibility and traceability via inline citations or URL references.\n",
        "- **Context-Aware**: Maintains conversation context to provide coherent multi-turn interactions.\n",
        "- **Readable Format**: Uses markdown elements such as headers, bullet points, and bold text for readability.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "3h2iGpE9d8VM"
      },
      "outputs": [],
      "source": [
        "def synthesize_answer_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Synthesizes a comprehensive answer from retrieved information.\n",
        "\n",
        "    Args:\n",
        "        state: Current state containing question and retrieved information\n",
        "\n",
        "    Returns:\n",
        "        Updated state with answer\n",
        "    \"\"\"\n",
        "    # Determine which source to use for synthesis\n",
        "    if state[\"arxiv_results\"] and len(state[\"arxiv_results\"]) > 0:\n",
        "        # Using ArXiv research papers\n",
        "        sources = \"\\n\\n\".join([\n",
        "            f\"--- Document: {d.metadata.get('source', 'Unknown')} (Page {d.metadata.get('page', 'Unknown')}) ---\\n{d.page_content}\"\n",
        "            for d in state[\"arxiv_results\"]\n",
        "        ])\n",
        "\n",
        "        print(boxen(sources, title=\">>> Sources\", color=\"blue\", padding=1))\n",
        "\n",
        "        prompt_template =\"\"\"\n",
        "        You are a knowledgeable research assistant specializing in mathematical theory and scientific literature analysis.\n",
        "\n",
        "        Your goal is to generate clean, formatted responses to user questions based solely on the provided ArXiv sources.\n",
        "\n",
        "        ---\n",
        "\n",
        "        Question:\n",
        "        {question}\n",
        "\n",
        "        Relevant Extracts from ArXiv Papers:\n",
        "        {sources}\n",
        "\n",
        "        Conversation History:\n",
        "        {conversation_history}\n",
        "\n",
        "        ---\n",
        "\n",
        "        Instructions for Synthesizing the Answer:\n",
        "\n",
        "        1. Read the extracts thoroughly and understand the concepts.\n",
        "        2. Answer the question comprehensively using only the provided context.\n",
        "        3. Organize the response into the following markdown sections (if applicable):\n",
        "          - Summary\n",
        "          - Key Concepts\n",
        "          - Theoretical Results\n",
        "          - Implications / Applications\n",
        "        4. Cite from the paper in the format: (Author et al., Page X). If page number is unknown, write: (Author et al.).\n",
        "        6. Avoid repetition, excessive formal tone, or generic commentary. Be clear and concise.**\n",
        "        7. If the provided text lacks enough detail to answer, state it clearly and suggest what additional info is needed.\n",
        "\n",
        "        ---\n",
        "\n",
        "        Now, write a well-structured, markdown-formatted answer to the question and it should be in a readable format as well.\n",
        "        \"\"\"\n",
        "    else:\n",
        "        # Using web search results\n",
        "        sources = \"\\n\\n\".join([\n",
        "            f\"--- Source {i+1}: {res['title']} ---\\n{res['content']}\"\n",
        "            for i, res in enumerate(state[\"web_results\"] or [])\n",
        "        ])\n",
        "\n",
        "        print(boxen(sources, title=\">>> Sources\", color=\"blue\", padding=1))\n",
        "\n",
        "        prompt_template = \"\"\"\n",
        "        You are a knowledgeable research assistant providing accurate information based on web search results.\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Here are relevant web search results:\n",
        "        {sources}\n",
        "\n",
        "        Conversation History:\n",
        "        {conversation_history}\n",
        "\n",
        "        Instructions:\n",
        "        1. Synthesize a comprehensive answer using ONLY the information provided above.\n",
        "        2. Cite sources using [1], [2], etc. corresponding to the source numbers above.\n",
        "        3. If the search results don't contain sufficient information, acknowledge the limitations.\n",
        "        4. DO NOT make up information not present in the sources.\n",
        "        5. Include only facts supported by the sources.\n",
        "\n",
        "        Your answer:\n",
        "        \"\"\"\n",
        "\n",
        "    # Create the prompt\n",
        "    synthesis_prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "\n",
        "    # Use a more capable model for synthesis\n",
        "    llm = ChatOpenAI(model=\"gpt-4-turbo\")\n",
        "    chain = synthesis_prompt | llm | StrOutputParser()\n",
        "\n",
        "    # Generate the answer\n",
        "    response = chain.invoke({\n",
        "        \"question\": state[\"question\"],\n",
        "        \"sources\": sources,\n",
        "        \"conversation_history\": state[\"conversation_history\"]\n",
        "    })\n",
        "\n",
        "    # Add source citations for web results\n",
        "    if state.get(\"web_results\") and not state.get(\"arxiv_results\"):\n",
        "        answer_content = response\n",
        "\n",
        "        # Add URL references at the end\n",
        "        url_citations = \"\\n\\nSources:\\n\" + \"\\n\".join([\n",
        "            f\"[{i+1}] {res['url']}\"\n",
        "            for i, res in enumerate(state[\"web_results\"] or [])\n",
        "        ])\n",
        "\n",
        "        answer_content += url_citations\n",
        "    else:\n",
        "        answer_content = response\n",
        "\n",
        "    # Format the output with clear separation between context and response\n",
        "    formatted_output = f\"\"\"\n",
        "{boxen(\"Context\", title=\">>> Context\", color=\"blue\", padding=1)}\n",
        "Question: {state[\"question\"]}\n",
        "Source: {'ArXiv Papers' if state.get(\"arxiv_results\") else 'Web Search Results'}\n",
        "\n",
        "\n",
        "{boxen(\"Response\", title=\">>> Response\", color=\"green\", padding=1)}\n",
        "{answer_content}\n",
        "\"\"\"\n",
        "\n",
        "    return {\"answer\": formatted_output}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePSZSMQBiaMC"
      },
      "source": [
        "## 7. Conversation Memory Node\n",
        "\n",
        "This node **manages conversation history** to provide context for **multi-turn interactions**.\n",
        "\n",
        "- **Stores** previous Q&A\n",
        "- **Updates** the state with the current interaction\n",
        "- **Maintains** a sliding window of relevant history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pgefbaa_eAfZ",
        "outputId": "c26b68be-0f78-4d3b-eb21-21a7f61e5901"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "# Initialize conversation memory\n",
        "memory = ConversationBufferMemory(return_messages=False, output_key=\"answer\", input_key=\"question\")\n",
        "\n",
        "def update_memory_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Updates the conversation memory with the current Q&A pair.\n",
        "\n",
        "    Args:\n",
        "        state: Current state with question and answer\n",
        "\n",
        "    Returns:\n",
        "        Updated state with new conversation_history\n",
        "    \"\"\"\n",
        "    # Save the current interaction to memory\n",
        "    memory.save_context(\n",
        "        {\"question\": state[\"question\"]},\n",
        "        {\"answer\": state[\"answer\"]}\n",
        "    )\n",
        "\n",
        "    # Get the updated conversation history\n",
        "    updated_history = memory.load_memory_variables({})[\"history\"]\n",
        "\n",
        "    # Return the updated state\n",
        "    return {\"conversation_history\": updated_history}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxKEeu64iglN"
      },
      "source": [
        "# Workflow State Graph Setup\n",
        "\n",
        "## Overview\n",
        "This section sets up the **LangGraph state machine** for managing the conversational agent’s workflow. It defines how user queries are processed step-by-step using modular nodes.\n",
        "\n",
        "---\n",
        "\n",
        "## Purpose\n",
        "To create a graph-based control flow that determines how the agent processes input, performs retrieval, synthesizes responses, updates memory, and eventually ends the workflow.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Components\n",
        "\n",
        "### 1. **Workflow Initialization**\n",
        "- A new `StateGraph` is initialized with the `AgentState` type, defining the structure of the workflow.\n",
        "\n",
        "### 2. **Node Definitions**\n",
        "The graph is composed of several functional nodes, each responsible for a specific task:\n",
        "- **router**: Determines whether to fetch data from the web or ArXiv.\n",
        "- **arxiv_retrieval**: Retrieves relevant research papers from ArXiv.\n",
        "- **web_search**: Retrieves web results via Tavily.\n",
        "- **synthesize**: Synthesizes a final answer from the retrieved information.\n",
        "- **update_memory**: Stores the interaction context for future turns.\n",
        "\n",
        "### 3. **Entry Point**\n",
        "- The `router` node is set as the initial entry point for the graph, meaning every workflow starts with routing logic.\n",
        "\n",
        "### 4. **Conditional Routing**\n",
        "- A conditional edge is established from `router` based on the `\"next\"` field in the state:\n",
        "  - If `\"next\"` is `\"web_search\"`, it routes to the `web_search` node.\n",
        "  - If `\"next\"` is `\"arxiv_retrieval\"`, it routes to the `arxiv_retrieval` node.\n",
        "\n",
        "### 5. **Workflow Sequence**\n",
        "The following fixed transitions define the remainder of the workflow:\n",
        "- From either `web_search` or `arxiv_retrieval` → go to `synthesize`\n",
        "- From `synthesize` → go to `update_memory`\n",
        "- From `update_memory` → reach `END` (completion of the flow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "BRYbjFhHeEZt"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "# Create the workflow state graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Add all nodes to the graph\n",
        "workflow.add_node(\"router\", router_node)\n",
        "workflow.add_node(\"arxiv_retrieval\", arxiv_retrieval_node)\n",
        "workflow.add_node(\"web_search\", web_search_node)\n",
        "workflow.add_node(\"synthesize\", synthesize_answer_node)\n",
        "workflow.add_node(\"update_memory\", update_memory_node)\n",
        "\n",
        "# Set entry point\n",
        "workflow.set_entry_point(\"router\")\n",
        "\n",
        "# Define conditional edges from router\n",
        "workflow.add_conditional_edges(\n",
        "    \"router\",\n",
        "    lambda state: state[\"next\"],\n",
        "    {\n",
        "        \"web_search\": \"web_search\",\n",
        "        \"arxiv_retrieval\": \"arxiv_retrieval\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Define rest of the edges\n",
        "workflow.add_edge(\"arxiv_retrieval\", \"synthesize\")\n",
        "workflow.add_edge(\"web_search\", \"synthesize\")\n",
        "workflow.add_edge(\"synthesize\", \"update_memory\")\n",
        "workflow.add_edge(\"update_memory\", END)\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m4TP3xSioHc"
      },
      "source": [
        "## 9. Testing the System\n",
        "\n",
        "Let's **test our system** with different types of questions:\n",
        "\n",
        "- **Questions answerable** from ArXiv papers\n",
        "- **Questions requiring** web search\n",
        "- **Follow-up questions** to test memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "gFoxsXLWeHSv"
      },
      "outputs": [],
      "source": [
        "def ask(question: str):\n",
        "    \"\"\"\n",
        "    Ask a question to the agentic RAG system.\n",
        "\n",
        "    Args:\n",
        "        question: User's question\n",
        "\n",
        "    Returns:\n",
        "        The system's answer\n",
        "    \"\"\"\n",
        "    # Print the question in a cyan box\n",
        "    print(boxen(f\"Question: {question}\", title=\">>> User Question\", color=\"cyan\", padding=1))\n",
        "\n",
        "    # Initialize the state\n",
        "    initial_state = {\n",
        "        \"question\": question,\n",
        "        \"arxiv_results\": None,\n",
        "        \"web_results\": None,\n",
        "        \"answer\": \"\",\n",
        "        \"conversation_history\": memory.load_memory_variables({}).get(\"history\", \"\")\n",
        "    }\n",
        "\n",
        "    # Invoke the workflow\n",
        "    result = app.invoke(initial_state)\n",
        "\n",
        "    # Split the answer into context and response sections\n",
        "    answer_parts = result[\"answer\"].split(\"\\n\\n\", 1)\n",
        "    if len(answer_parts) == 2:\n",
        "        context, response = answer_parts\n",
        "    else:\n",
        "        context = \"No specific context provided\"\n",
        "        response = result[\"answer\"]\n",
        "\n",
        "    # Print the response in a green box\n",
        "    print(boxen(response, title=\">>> Response\", color=\"green\", padding=1))\n",
        "\n",
        "    return result[\"answer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b0Ps7Nw1exMl",
        "outputId": "2a91d4e2-ee73-4533-c785-d6e8d86fdd81"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m╭─\u001b[0m\u001b[36m >>> User Question \u001b[0m\u001b[36m──────────────────────────────────────────────────────────────\u001b[0m\u001b[36m─╮\u001b[0m                              \n",
            "\u001b[36m│\u001b[0m                                                                                   \u001b[36m│\u001b[0m                              \n",
            "\u001b[36m│\u001b[0m   Question: Explain the surface code implementation in quantum error correction   \u001b[36m│\u001b[0m                              \n",
            "\u001b[36m│\u001b[0m                                                                                   \u001b[36m│\u001b[0m                              \n",
            "\u001b[36m╰───────────────────────────────────────────────────────────────────────────────────╯\u001b[0m                              \n",
            "\n",
            "Router decision: arxiv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m╭─\u001b[0m\u001b[33m >>> Retrieval Results \u001b[0m\u001b[33m────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m                                                                \n",
            "\u001b[33m│\u001b[0m                                                 \u001b[33m│\u001b[0m                                                                \n",
            "\u001b[33m│\u001b[0m   Found 5 relevant chunks above threshold 0.5   \u001b[33m│\u001b[0m                                                                \n",
            "\u001b[33m│\u001b[0m                                                 \u001b[33m│\u001b[0m                                                                \n",
            "\u001b[33m╰─────────────────────────────────────────────────╯\u001b[0m                                                                \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m╭─\u001b[0m\u001b[34m >>> Sources \u001b[0m\u001b[34m────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[34m─╮\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m                                                                                               \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   --- Document: Unknown (Page Unknown) ---                                                    \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   cle Conﬁgurations with Targeted SpectralFunctions via Eﬀe ctive Pair Interactions,.         \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   Physical Review E, 101 032124, 2020.                                                        \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   38. G. F. Us, A truncated symmetric generalized power moment problem. Ukrain. Mat.          \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   Z. 26 (1974) 348–358, 429.                                                                  \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   R.E. Curto, Department of Mathematics, University of Iowa, Iowa City,                       \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   52246, USA                                                                                  \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   Email address : raul-curto@uiowa.edu                                                        \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   M. Infusino, Dipartimento di Matematica e Informatica, Uni versit´a degli                   \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   Studi di Cagliari, Palazzo delle Scienze, Via Ospedale 72, 0 9124 Cagliari                  \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   Email address : maria.infusino@unica.it                                                     \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m                                                                                               \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   --- Document: Unknown (Page Unknown) ---                                                    \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   eral theory. Inﬁnite Dimensional Analysis, Quantum Probability and Rel ated Topics ,        \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   5(2):201–233, 2002.                                                                         \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   22. Yu. G. Kondratiev, T. Kuna, and M. J. Oliveira. Holomorph ic Bogoliubov functionals     \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   for interacting particle systems in continuum. J. Funct. Anal. , 238(2):375–404, 2006.      \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   23. L. Koralov. Existence of pair potential corresponding t o speciﬁed density and pair     \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   correlation. Lett. Math. Phys. , 71(2):135–148, 2005.                                       \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   24. K. Krickeberg. Moments of point processes. pages 70–101 . Lecture Notes in Math.,       \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   Vol. 296, 1973.                                                                             \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   25. H. Kummer. n-Representability Problem for Reduced Dens ity Matrices. Journal of         \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   Mathematical Physics. 8(10): 2063– 2081, 1967.                                              \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   26. T. Kuna, J. Lebowitz, E. Speer. Realizability of Point Pr ocesses. J. Stat. Phys.       \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   129(3):417–439, 2007.                                                                       \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   27. T. Kuna, J. L. Lebowitz, E. R. Speer, Necessary and suﬃcie nt conditions for realiz-    \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   ability of point processes. Ann. Appl. Probab. 21 (2011), 1253–1281.                        \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m                                                                                               \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   --- Document: Unknown (Page Unknown) ---                                                    \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   of Dirac measures concentrated at the points of a sequence in X without                     \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   accumulation points, i.e., η is a Radon measure on the space X. Hence, the                  \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   set N (X) of all possible conﬁgurations of the system components is a subset                \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   of the vector space M(X) of all signed Radon measures on X. Prescribing                     \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   the correlation functions of a probability measure on N (X) up to some order                \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   n means specifying its factorial moments up to order n, which is equivalent to              \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   specifying its power moments up to order n, and so the realizability problem                \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   can be interpreted as a truncated K−moment problem with K ⊆ N (X).                          \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m                                                                                               \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   --- Document: Unknown (Page Unknown) ---                                                    \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   4 R.E. CURTO AND M. INFUSINO                                                                \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   measure ensures that any compact set ofX contains only ﬁnitely many sys-                    \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   tem components. The space N (X) of all point conﬁgurations is known as                      \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   point conﬁguration space on X (see, e.g., [21]) and is a closed subset of the               \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   space M(X) of all signed Radon measures supported in X endowed with                         \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   the so-called vague topology τ, i.e., the weakest topology on M(X) such                     \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   the map M(X) → R, ν ↦→                                                                      \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   ∫                                                                                           \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   X fdµ is continuous for all f ∈ C c(X), where                                               \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   Cc(X) denotes the space of all continuous real-valued functions compactly                   \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   supported in X (see [20, Lemma 4.4]).                                                       \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   A point process on X is a Radon probability measure on M(X) which is                        \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   supported in N (X) and intuitively is a random distribution of points in X                  \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   such that, with probability one, any compact set contains on ly ﬁnitely many                \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   of these points (see, e.g. [9] for an overview).                                            \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   Given a point process µ on X, its nth correlation function ρ(n)                             \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   µ is deﬁned                                                                                 \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   as the expected value of the nth factorial power η⊙n of an element η ∈ N (X),               \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m                                                                                               \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   --- Document: Unknown (Page Unknown) ---                                                    \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   7. E. Caglioti, M. Infusino, T. Kuna, Translation invariant realizability problem on the    \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   d-dimensional lattice: an explicit construction. Electro n. Commun. Probab. 21 (2016),      \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   Paper No. 45, 9 pp.                                                                         \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   8. R. E. Curto, M. Ghasemi, M. Infusino, S. Kuhlmann, The trun cated moment                 \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   problem for Unital Commutative R− algebras, to appear in J. Operator Theory                 \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   https://arxiv.org/abs/2009.05115                                                            \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   9. D. J. Daley, D. Vere-Jones, An Introduction to the Theory o f Point Processes. Vol. I:   \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   Elementary Theory and Methods, 2nd ed. Springer, New York, 2 003.                           \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   10. R. M. Erdahl. Representability, Int. J. Quantum Chem. 13 : 697–718, 1978.               \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   11. C. Garrod and J.K. Percus, Reduction of the N-particle variational problem, J. Math.    \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m   Phys. 5(1964), 1756–1776.                                                                   \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m│\u001b[0m                                                                                               \u001b[34m│\u001b[0m                  \n",
            "\u001b[34m╰───────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m                  \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['\\n\\x1b[34m╭─\\x1b[0m\\x1b[34m >>> Context \\x1b[0m\\x1b[34m─╮\\x1b[0m                                                                                                  \\n\\x1b[34m│\\x1b[0m               \\x1b[34m│\\x1b[0m                                                                                                  \\n\\x1b[34m│\\x1b[0m    Context    \\x1b[34m│\\x1b[0m                                                                                                  \\n\\x1b[34m│\\x1b[0m               \\x1b[34m│\\x1b[0m                                                                                                  \\n\\x1b[34m╰───────────────╯\\x1b[0m                                                                                                  ', 'Question: Explain the surface code implementation in quantum error correction\\nSource: ArXiv Papers\\n\\n\\n\\x1b[32m╭─\\x1b[0m\\x1b[32m >>> Response \\x1b[0m\\x1b[32m─╮\\x1b[0m                                                                                                 \\n\\x1b[32m│\\x1b[0m                \\x1b[32m│\\x1b[0m                                                                                                 \\n\\x1b[32m│\\x1b[0m    Response    \\x1b[32m│\\x1b[0m                                                                                                 \\n\\x1b[32m│\\x1b[0m                \\x1b[32m│\\x1b[0m                                                                                                 \\n\\x1b[32m╰────────────────╯\\x1b[0m                                                                                                 \\n\\n### Summary\\n\\nThe provided excerpts from ArXiv papers do not encompass specific details on the implementation of surface codes in quantum error correction. These excerpts instead predominantly discuss mathematical theories related to probability measures, Radon measures, and realizability of point processes, which are distinct from the specific domain of quantum error correction using surface codes.\\n\\n### Key Concepts\\n\\n- **Point Processes and Radon Measures:** The excerpts delve into the statistical treatment of point configurations and their measures. These concepts are focused on the mathematical structure and constraints of point distribution in prescribed spaces (Curto et al.).\\n\\n- **Realizability of Systems:** There is significant mention of the realizability problem concerning how certain desired properties or configurations can be realized in mathematical models (Curto et al.).\\n\\n### Theoretical Results\\n\\nWhile there are no direct mentions of surface codes, the general theory concerning point processes and measures could potentially apply to broader aspects of quantum systems where the spatial distribution and stochastic processes are of interest. The exact connection, however, is unexplored within the provided excerpts.\\n\\n### Implications / Applications\\n\\nThe implications or applications of the discussed theories in quantum error correction using surface codes are not directly articulated in the provided materials. Surface code itself specifically involves arranging qubits in a two-dimensional lattice and employing a combination of X and Z measurements to correct errors efficiently, a topic that requires specific quantum computing context not covered in the excerpts.\\n\\n### Conclusion\\n\\nThe theoretical discussions in the provided documents do not address the query about the implementation of surface codes in quantum error correction. To effectively discuss this, additional literature specifically covering quantum computing, error correction codes, and probably more detailed information on the surface code techniques, are necessary. \\n\\nFor accurate and detailed information on surface codes in quantum error correction, sources specifically covering quantum computing mechanisms, error correction theory, and experimental quantum computing results should be referenced.\\n']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m╭─\u001b[0m\u001b[32m >>> Response \u001b[0m\u001b[32m─────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   Question: Explain the surface code implementation in quantum error correction                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   Source: ArXiv Papers                                                                                          \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   \u001b[32m╭─\u001b[0m\u001b[32m >>> Response \u001b[0m\u001b[32m─╮\u001b[0m                                                                       \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   \u001b[32m│\u001b[0m                \u001b[32m│\u001b[0m                                                                              \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   \u001b[32m│\u001b[0m    Response    \u001b[32m│\u001b[0m                                                                              \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   \u001b[32m│\u001b[0m                \u001b[32m│\u001b[0m                                                                              \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   \u001b[32m╰────────────────╯\u001b[0m                                                                                     \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   ### Summary                                                                                                   \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   The provided excerpts from ArXiv papers do not encompass specific details on the implementation of surface    \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   codes in quantum error correction. These excerpts instead predominantly discuss mathematical theories         \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   related to probability measures, Radon measures, and realizability of point processes, which are distinct     \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   from the specific domain of quantum error correction using surface codes.                                     \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   ### Key Concepts                                                                                              \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   - **Point Processes and Radon Measures:** The excerpts delve into the statistical treatment of point          \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   configurations and their measures. These concepts are focused on the mathematical structure and constraints   \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   of point distribution in prescribed spaces (Curto et al.).                                                    \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   - **Realizability of Systems:** There is significant mention of the realizability problem concerning how      \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   certain desired properties or configurations can be realized in mathematical models (Curto et al.).           \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   ### Theoretical Results                                                                                       \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   While there are no direct mentions of surface codes, the general theory concerning point processes and        \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   measures could potentially apply to broader aspects of quantum systems where the spatial distribution and     \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   stochastic processes are of interest. The exact connection, however, is unexplored within the provided        \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   excerpts.                                                                                                     \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   ### Implications / Applications                                                                               \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   The implications or applications of the discussed theories in quantum error correction using surface codes    \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   are not directly articulated in the provided materials. Surface code itself specifically involves arranging   \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   qubits in a two-dimensional lattice and employing a combination of X and Z measurements to correct errors     \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   efficiently, a topic that requires specific quantum computing context not covered in the excerpts.            \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   ### Conclusion                                                                                                \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   The theoretical discussions in the provided documents do not address the query about the implementation of    \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   surface codes in quantum error correction. To effectively discuss this, additional literature specifically    \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   covering quantum computing, error correction codes, and probably more detailed information on the surface     \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   code techniques, are necessary.                                                                               \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   For accurate and detailed information on surface codes in quantum error correction, sources specifically      \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   covering quantum computing mechanisms, error correction theory, and experimental quantum computing results    \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   should be referenced.                                                                                         \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n\\x1b[34m╭─\\x1b[0m\\x1b[34m >>> Context \\x1b[0m\\x1b[34m─╮\\x1b[0m                                                                                                  \\n\\x1b[34m│\\x1b[0m               \\x1b[34m│\\x1b[0m                                                                                                  \\n\\x1b[34m│\\x1b[0m    Context    \\x1b[34m│\\x1b[0m                                                                                                  \\n\\x1b[34m│\\x1b[0m               \\x1b[34m│\\x1b[0m                                                                                                  \\n\\x1b[34m╰───────────────╯\\x1b[0m                                                                                                  \\n\\nQuestion: Explain the surface code implementation in quantum error correction\\nSource: ArXiv Papers\\n\\n\\n\\x1b[32m╭─\\x1b[0m\\x1b[32m >>> Response \\x1b[0m\\x1b[32m─╮\\x1b[0m                                                                                                 \\n\\x1b[32m│\\x1b[0m                \\x1b[32m│\\x1b[0m                                                                                                 \\n\\x1b[32m│\\x1b[0m    Response    \\x1b[32m│\\x1b[0m                                                                                                 \\n\\x1b[32m│\\x1b[0m                \\x1b[32m│\\x1b[0m                                                                                                 \\n\\x1b[32m╰────────────────╯\\x1b[0m                                                                                                 \\n\\n### Summary\\n\\nThe provided excerpts from ArXiv papers do not encompass specific details on the implementation of surface codes in quantum error correction. These excerpts instead predominantly discuss mathematical theories related to probability measures, Radon measures, and realizability of point processes, which are distinct from the specific domain of quantum error correction using surface codes.\\n\\n### Key Concepts\\n\\n- **Point Processes and Radon Measures:** The excerpts delve into the statistical treatment of point configurations and their measures. These concepts are focused on the mathematical structure and constraints of point distribution in prescribed spaces (Curto et al.).\\n\\n- **Realizability of Systems:** There is significant mention of the realizability problem concerning how certain desired properties or configurations can be realized in mathematical models (Curto et al.).\\n\\n### Theoretical Results\\n\\nWhile there are no direct mentions of surface codes, the general theory concerning point processes and measures could potentially apply to broader aspects of quantum systems where the spatial distribution and stochastic processes are of interest. The exact connection, however, is unexplored within the provided excerpts.\\n\\n### Implications / Applications\\n\\nThe implications or applications of the discussed theories in quantum error correction using surface codes are not directly articulated in the provided materials. Surface code itself specifically involves arranging qubits in a two-dimensional lattice and employing a combination of X and Z measurements to correct errors efficiently, a topic that requires specific quantum computing context not covered in the excerpts.\\n\\n### Conclusion\\n\\nThe theoretical discussions in the provided documents do not address the query about the implementation of surface codes in quantum error correction. To effectively discuss this, additional literature specifically covering quantum computing, error correction codes, and probably more detailed information on the surface code techniques, are necessary. \\n\\nFor accurate and detailed information on surface codes in quantum error correction, sources specifically covering quantum computing mechanisms, error correction theory, and experimental quantum computing results should be referenced.\\n'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test with a question about quantum computing (should use ArXiv)\n",
        "ask(\"Explain the surface code implementation in quantum error correction\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M_IbwltMg2l8",
        "outputId": "870c1408-585e-413b-9201-011cd4ad8a63"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m╭─\u001b[0m\u001b[36m >>> User Question \u001b[0m\u001b[36m───────────────────────────────────────────\u001b[0m\u001b[36m─╮\u001b[0m                                                 \n",
            "\u001b[36m│\u001b[0m                                                                \u001b[36m│\u001b[0m                                                 \n",
            "\u001b[36m│\u001b[0m   Question: How does this compare to topological approaches?   \u001b[36m│\u001b[0m                                                 \n",
            "\u001b[36m│\u001b[0m                                                                \u001b[36m│\u001b[0m                                                 \n",
            "\u001b[36m╰────────────────────────────────────────────────────────────────╯\u001b[0m                                                 \n",
            "\n",
            "Router decision: arxiv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m╭─\u001b[0m\u001b[33m >>> Retrieval Results \u001b[0m\u001b[33m────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m                                                                \n",
            "\u001b[33m│\u001b[0m                                                 \u001b[33m│\u001b[0m                                                                \n",
            "\u001b[33m│\u001b[0m   Found 5 relevant chunks above threshold 0.5   \u001b[33m│\u001b[0m                                                                \n",
            "\u001b[33m│\u001b[0m                                                 \u001b[33m│\u001b[0m                                                                \n",
            "\u001b[33m╰─────────────────────────────────────────────────╯\u001b[0m                                                                \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m╭─\u001b[0m\u001b[34m >>> Sources \u001b[0m\u001b[34m───────────────────────────────────────────────────────\u001b[0m\u001b[34m─╮\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m                                                                      \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   --- Document: Unknown (Page Unknown) ---                           \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   multiple ways of achievingthe same immediate goal, which           \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   is prevalent in human demonstration data Mandlekar et al.          \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   (2021). In Fig 3, we present a case study of this type of short-   \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   horizon multimodality in the Push-T task. Diffusion Policy         \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   learns to approach the contact point equally likely from left      \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   or right, while LSTM-GMM Mandlekar et al. (2021) and               \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   IBC Florence et al. (2021) exhibit bias toward one side and        \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   BET Shafiullah et al. (2022) cannot commit to one mode.            \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   Diffusion Policy can express long-horizon multimodal-              \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   ity. Long-horizon multimodality is the completion of differ-       \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   ent sub-goalsin inconsistent order. For example, the order of      \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   pushing a particular block in the Block Push task or the order     \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   of interacting with 7 possible objects in the Kitchen task are     \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   arbitrary. We find that Diffusion Policy copes well with this      \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   type of multimodality; it outperforms baselines on both tasks      \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   by a large margin: 32% improvement on Block Push’s p2              \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m                                                                      \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   --- Document: Unknown (Page Unknown) ---                           \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   Florence et al. (2021). All state-based tasks are trained for      \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   4500 epochs, and image-based tasks for 3000 epochs. Each           \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   method is evaluated with its best-performing action space:         \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   position control for Diffusion Policy and velocity control         \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   for baselines (the effect of action space will be discussed        \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   in detail in Sec 5.3). The results from these simulation           \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   benchmarks are summarized in Table 1 and Table 2.                  \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   5.3 Key Findings                                                   \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   Diffusion Policy outperforms alternative methods on all tasks      \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   and variants, with both state and vision observations, in          \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   our simulation benchmark study (Tabs 1, 2 and 4) with an           \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   average improvement of 46.9%. The following paragraphs             \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   summarize the key takeaways.                                       \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   Diffusion Policy can express short-horizon multi-                  \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   modality. We define short-horizon action multimodality as          \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   multiple ways of achievingthe same immediate goal, which           \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   is prevalent in human demonstration data Mandlekar et al.          \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   (2021). In Fig 3, we present a case study of this type of short-   \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m                                                                      \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   --- Document: Unknown (Page Unknown) ---                           \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   all steps. We threshold success rate by the minimum achieved       \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   IoU metric from the human demonstration dataset. Our UR5-          \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   based experiment setup is shown in Fig 6. Diffusion Policy         \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   predicts robot commands at 10 Hz and these commands then           \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   linearly interpolated to 125 Hz for robot execution.               \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   Result Analysis. Diffusion Policy performed close to               \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   human level with 95% success rate and 0.8 v.s. 0.84                \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   average IoU, compared with the 0% and 20% success rate             \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   of best-performing IBC and LSTM-GMM variants. Fig 7                \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   qualitatively illustrates the behavior for each method starting    \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   from the same initial condition. We observed that poor             \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   performance during the transition between stages is the            \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   most common failure case for the baseline method due to            \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   high multimodality during those sections and an ambiguous          \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   decision boundary. LSTM-GMM got stuck near the T block             \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   in 8 out of 20 evaluations (3rd row), while IBC prematurely        \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   left the T block in 6 out of 20 evaluations (4th row).             \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m                                                                      \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   --- Document: Unknown (Page Unknown) ---                           \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   we demonstrate that diffusion-based visuomotor policies            \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   consistently and definitively outperform existing methods          \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   while also being stable and easy to train. Our results also        \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   highlight critical design factors, including receding-horizon      \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   action prediction, end-effector position control, and efficient    \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   visual conditioning, that is crucial for unlocking the full        \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   potential of diffusion-based policies. While many factors          \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   affect the ultimate quality of behavior-cloned policies —          \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   including the quality and quantity of demonstrations, the          \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   physical capabilities of the robot, the policy architecture,       \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   and the pretraining regime used — our experimental results         \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   strongly indicate that policy structure poses a significant        \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   performance bottleneck during behavior cloning. We hope            \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   that this work drives further exploration in the field into        \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   diffusion-based policies and highlights the importance of          \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   considering all aspects of the behavior cloning process            \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   beyond just the data used for policy training.                     \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m                                                                      \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   --- Document: Unknown (Page Unknown) ---                           \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   2015; Ho et al. 2020) have recently been applied to solve          \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   various different control tasks (Janner et al. 2022a; Urain        \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   et al. 2022; Ajay et al. 2022).                                    \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   In particular, Janner et al. (2022a) and Huang et al.              \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   (2023) explore how diffusion models may be used in the             \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   context of planning and infer a trajectory of actions that         \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   may be executed in a given environment. In the context of          \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   Reinforcement Learning, Wang et al. (2022) use diffusion           \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   model for policy representation and regularization with state-     \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   based observations. In contrast, in this work, we explore          \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   how diffusion models may instead be effectively applied in         \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   the context of behavioral cloning for effective visuomotor         \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   control policy. To construct effective visuomotor control          \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   policies, we propose to combine DDPM’s ability to predict          \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   high-dimensional action squences with closed-loop control,         \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   as well as a new transformer architecture for action diffusion     \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   and a manner to integrate visual inputs into the action            \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m   diffusion model.                                                   \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m│\u001b[0m                                                                      \u001b[34m│\u001b[0m                                           \n",
            "\u001b[34m╰──────────────────────────────────────────────────────────────────────╯\u001b[0m                                           \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m╭─\u001b[0m\u001b[32m >>> Response \u001b[0m\u001b[32m─────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   Question: How does this compare to topological approaches?                                                    \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   Source: ArXiv Papers                                                                                          \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   \u001b[32m╭─\u001b[0m\u001b[32m >>> Response \u001b[0m\u001b[32m─╮\u001b[0m                                                                       \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   \u001b[32m│\u001b[0m                \u001b[32m│\u001b[0m                                                                              \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   \u001b[32m│\u001b[0m    Response    \u001b[32m│\u001b[0m                                                                              \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   \u001b[32m│\u001b[0m                \u001b[32m│\u001b[0m                                                                              \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   \u001b[32m╰────────────────╯\u001b[0m                                                                                     \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   ### Summary                                                                                                   \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   The comparison between diffusion-based approaches and topological methods in handling tasks with multimodal   \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   action possibilities suggests that diffusion policies are particularly adept at expressing both               \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   short-horizon and long-horizon multimodality. This capability allows them to perform well in tasks where      \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   multiple routes or sequences of actions can achieve the same end goal.                                        \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   ### Key Concepts                                                                                              \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   - **Short-horizon Multimodality**: Diffusion policies excel in short-horizon multimodality, allowing          \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   different approaches to achieve the same immediate goal, such as approaching a contact point from either      \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   side in a manipulation task.                                                                                  \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   - **Long-horizon Multimodality**: These policies also efficiently handle long-horizon multimodality, where    \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   tasks can be completed in varying orders without affecting the final outcome. This is crucial in complex      \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   scenarios like interacting with multiple objects where the sequence of actions is non-deterministic.          \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   ### Theoretical Results                                                                                       \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   The Diffusion Policy method has shown significant performance improvements:                                   \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   - **Quantitative Gains**: It outperforms traditional methods like LSTM-GMM and IBC with notable margins;      \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   for instance, a 32% improvement on the Block Push task (Diffusion Policy).                                    \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   - **Application Efficiency**: These policies operate effectively across both vision and state-based tasks,    \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   significantly outperforming baselines in all evaluated scenarios (Diffusion Policy).                          \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   ### Implications / Applications                                                                               \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   In direct comparison to topological approaches, diffusion policies might offer several advantages:            \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   - **Flexibility in Action Sequencing**: Unlike some topological methods, which might require more rigid       \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   sequencing of actions or paths, diffusion policies thrive in environments where such rigidity can be a        \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   disadvantage.                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   - **Handling of Complex Environments**: The ability to interact with multiple elements in varying orders      \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   without a predetermined path suggests a robust advantage over topological methods which might struggle in     \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   highly dynamic or unpredictable environments.                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   These capabilities make diffusion methods particularly suited for real-world applications where variability   \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   and unpredictability can render rigidly defined topological methods less effective.                           \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n\\x1b[34m╭─\\x1b[0m\\x1b[34m >>> Context \\x1b[0m\\x1b[34m─╮\\x1b[0m                                                                                                  \\n\\x1b[34m│\\x1b[0m               \\x1b[34m│\\x1b[0m                                                                                                  \\n\\x1b[34m│\\x1b[0m    Context    \\x1b[34m│\\x1b[0m                                                                                                  \\n\\x1b[34m│\\x1b[0m               \\x1b[34m│\\x1b[0m                                                                                                  \\n\\x1b[34m╰───────────────╯\\x1b[0m                                                                                                  \\n\\nQuestion: How does this compare to topological approaches?\\nSource: ArXiv Papers\\n\\n\\n\\x1b[32m╭─\\x1b[0m\\x1b[32m >>> Response \\x1b[0m\\x1b[32m─╮\\x1b[0m                                                                                                 \\n\\x1b[32m│\\x1b[0m                \\x1b[32m│\\x1b[0m                                                                                                 \\n\\x1b[32m│\\x1b[0m    Response    \\x1b[32m│\\x1b[0m                                                                                                 \\n\\x1b[32m│\\x1b[0m                \\x1b[32m│\\x1b[0m                                                                                                 \\n\\x1b[32m╰────────────────╯\\x1b[0m                                                                                                 \\n\\n### Summary\\n\\nThe comparison between diffusion-based approaches and topological methods in handling tasks with multimodal action possibilities suggests that diffusion policies are particularly adept at expressing both short-horizon and long-horizon multimodality. This capability allows them to perform well in tasks where multiple routes or sequences of actions can achieve the same end goal.\\n\\n### Key Concepts\\n\\n- **Short-horizon Multimodality**: Diffusion policies excel in short-horizon multimodality, allowing different approaches to achieve the same immediate goal, such as approaching a contact point from either side in a manipulation task.\\n- **Long-horizon Multimodality**: These policies also efficiently handle long-horizon multimodality, where tasks can be completed in varying orders without affecting the final outcome. This is crucial in complex scenarios like interacting with multiple objects where the sequence of actions is non-deterministic.\\n\\n### Theoretical Results\\n\\nThe Diffusion Policy method has shown significant performance improvements:\\n- **Quantitative Gains**: It outperforms traditional methods like LSTM-GMM and IBC with notable margins; for instance, a 32% improvement on the Block Push task (Diffusion Policy).\\n- **Application Efficiency**: These policies operate effectively across both vision and state-based tasks, significantly outperforming baselines in all evaluated scenarios (Diffusion Policy).\\n\\n### Implications / Applications\\n\\nIn direct comparison to topological approaches, diffusion policies might offer several advantages:\\n- **Flexibility in Action Sequencing**: Unlike some topological methods, which might require more rigid sequencing of actions or paths, diffusion policies thrive in environments where such rigidity can be a disadvantage.\\n- **Handling of Complex Environments**: The ability to interact with multiple elements in varying orders without a predetermined path suggests a robust advantage over topological methods which might struggle in highly dynamic or unpredictable environments.\\n\\nThese capabilities make diffusion methods particularly suited for real-world applications where variability and unpredictability can render rigidly defined topological methods less effective.\\n'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# %% [code]\n",
        "# Test with a follow-up question (tests memory)\n",
        "ask(\"How does this compare to topological approaches?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TtHrxeMo1mr"
      },
      "source": [
        "## 🎓 Conclusion\n",
        "\n",
        "The Agentic RAG System with ArXiv + Web Fallback represents a powerful approach to information retrieval and synthesis, combining the best of both academic and real-time knowledge sources. By intelligently routing queries and maintaining conversation context, it provides:\n",
        "\n",
        "- **Comprehensive Answers**: Leveraging both academic papers and current web information\n",
        "- **Proper Attribution**: Ensuring all sources are properly cited\n",
        "- **Contextual Understanding**: Maintaining conversation history for coherent interactions\n",
        "- **Flexible Knowledge Access**: Adapting to different types of queries and information needs\n",
        "\n",
        "This system is particularly valuable for:\n",
        "- Researchers seeking both theoretical foundations and practical applications\n",
        "- Developers looking for up-to-date technical information\n",
        "- Students and professionals needing comprehensive, well-sourced answers\n",
        "- Anyone requiring a balance between academic rigor and current information\n",
        "\n",
        "The modular architecture and use of LangGraph make it easy to extend and adapt the system for specific use cases or additional knowledge sources."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
