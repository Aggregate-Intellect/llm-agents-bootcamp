{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3sQoe2Tg_fb"
      },
      "source": [
        "# Agentic RAG System with ArXiv + Web Fallback\n",
        "\n",
        "This notebook implements an intelligent research assistant that uses:\n",
        "1. **ArXiv papers** as the primary knowledge source (**RAG approach**)\n",
        "2. **Web search** as a fallback mechanism (**Tavily API**)\n",
        "3. **LangGraph** for orchestrating the decision-making workflow\n",
        "\n",
        "---\n",
        "\n",
        "## **Requirements**\n",
        "- **OpenAI API Key**\n",
        "- **Tavily API Key** (Get from [Tavily](https://app.tavily.com))\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Environment Setup**\n",
        "\n",
        "First, we'll install all necessary dependencies and set up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0zyWACuhcfqG",
        "outputId": "34aa52e5-c247-45a6-bde6-c7ee01a50282"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.6/91.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m643.9/643.9 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# %% [code]\n",
        "# Install required packages\n",
        "! pip install -qU langchain langgraph pypdf chromadb tavily-python openai python-dotenv pyboxen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ai9xIIrecwKb",
        "outputId": "8ba1fb89-083f-4ab9-bca6-9858a693fbbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain_chroma\n",
            "  Downloading langchain_chroma-0.2.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (0.3.51)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.12-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain_text_splitters in /usr/local/lib/python3.11/dist-packages (0.3.8)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.3.29)\n",
            "Requirement already satisfied: tavily-python in /usr/local/lib/python3.11/dist-packages (0.5.4)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.72.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.23)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.24)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Collecting numpy<3,>=1.26.2 (from langchain-community)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0 (from langchain_chroma)\n",
            "  Downloading chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (4.13.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (2.11.2)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.0.24)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.8)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.61)\n",
            "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from tavily-python) (0.28.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.2.2.post1)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.7.6)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.115.9)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.34.0)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.24.1)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.21.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.32.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.32.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.53b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.32.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.21.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.71.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.15.2)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (32.0.1)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.10.16)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (13.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->tavily-python) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->tavily-python) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->tavily-python) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.9.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.2.0)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.95.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.45.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.2.2)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.9)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.69.2)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.32.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.32.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.32.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.32.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.53b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.53b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.53b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.53b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.53b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.53b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.53b0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.53b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.17.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.53b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.30.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.5.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.6.4)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.0.5)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2025.3.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.1.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.6.1)\n",
            "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_chroma-0.2.2-py3-none-any.whl (11 kB)\n",
            "Downloading langchain_openai-0.3.12-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-0.6.3-py3-none-any.whl (611 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: numpy, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_openai, langchain-community, chromadb, langchain_chroma\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: chromadb\n",
            "    Found existing installation: chromadb 1.0.4\n",
            "    Uninstalling chromadb-1.0.4:\n",
            "      Successfully uninstalled chromadb-1.0.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chromadb-0.6.3 dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.21 langchain_chroma-0.2.2 langchain_openai-0.3.12 marshmallow-3.26.1 mypy-extensions-1.0.0 numpy-1.26.4 pydantic-settings-2.8.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "! pip install langchain-community langchain_chroma langchain_core langchain_openai langchain_text_splitters langgraph tavily-python openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MDJAvAUpcjmN"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "# Import required libraries\n",
        "import os  # Provides functions to interact with the operating system.\n",
        "from pyboxen import boxen  # Used to display stylized boxes in the terminal for better CLI UI.\n",
        "from getpass import getpass  # Allows secure password input without echoing.\n",
        "from typing import TypedDict, List, Dict, Optional, Literal, Union, Annotated, cast  # Used for type annotations and static type checking.\n",
        "from langchain_core.documents import Document  # Represents and structures text data in LangChain.\n",
        "from langchain_core.output_parsers import StrOutputParser  # Parses raw LLM output into usable string format.\n",
        "from langchain_community.document_loaders import PyPDFLoader  # Loads and extracts text from PDF documents.\n",
        "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter  # Splits text into chunks using markdown headers or character limits.\n",
        "from langchain_chroma import Chroma  # Provides integration with Chroma vector store for embedding storage and retrieval.\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI  # Interfaces with OpenAI for embeddings and chat models.\n",
        "from langchain_core.prompts import ChatPromptTemplate  # Manages prompt templates for chat-based interactions.\n",
        "from langgraph.graph import StateGraph, END  # Helps define state-based logic flows for chat systems.\n",
        "from tavily import TavilyClient  # Interfaces with Tavily for real-time web search.\n",
        "from langchain.memory import ConversationBufferMemory  # Maintains memory of past conversation for context retention.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API Key Submission\n",
        "\n",
        "Please follow the instructions below:\n",
        "\n",
        "1. **Provide the Tavily API Key**\n",
        "2. **Provide the Open API Key**\n",
        "3. **Press Enter** to proceed\n"
      ],
      "metadata": {
        "id": "aOl5svrxoDkQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "y1CrD7yecpRF",
        "outputId": "64a7a260-cffc-45f4-88e2-120ccb24f0be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Tavily API Key (get from https://app.tavily.com): ··········\n",
            "Enter OpenAI API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "# Set API keys\n",
        "os.environ[\"TAVILY_API_KEY\"] = getpass(\"Enter Tavily API Key (get from https://app.tavily.com): \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVsoCt4BhfRp"
      },
      "source": [
        "## 2. Define State and System Architecture\n",
        "\n",
        "We'll define our system's state and flow using **LangGraph**. The state will track our:\n",
        "- **Input question**\n",
        "- **Retrieved ArXiv results**\n",
        "- **Web search results**\n",
        "- **Final answer**\n",
        "- **Conversation history for context**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9DX0VBugddBN"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "# Define our system state - this is what passes between nodes in our graph\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"State definition for our agentic RAG system\"\"\"\n",
        "    question: str  # User's current question\n",
        "    arxiv_results: Optional[List[Document]]  # Results from ArXiv papers (if any)\n",
        "    web_results: Optional[List[Dict]]  # Results from web search (if any)\n",
        "    answer: str  # Final synthesized answer\n",
        "    conversation_history: str  # Previous Q&A for context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym7UTPc4hpli"
      },
      "source": [
        "## 3. Router Node Implementation\n",
        "\n",
        "The **Router Node** is responsible for deciding whether to use **ArXiv papers** or **web search**.\n",
        "- **First**, it tries to use **ArXiv papers** (our local knowledge source).\n",
        "- **Falls back** to **web search** if needed.\n",
        "\n",
        "This demonstrates **strategic decision-making capabilities**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4oSbkgqwdh_R"
      },
      "outputs": [],
      "source": [
        "router_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a highly specialized research assistant with access to two information sources:\n",
        "1. A collection of ArXiv research papers\n",
        "2. A web search tool\n",
        "\n",
        "Your task is to determine which source would be better to answer the user's question.\n",
        "FIRST try to use ArXiv papers for scientific and academic questions.\n",
        "ONLY use web search if:\n",
        "- The question requires very recent information not likely in research papers\n",
        "- The question is about general knowledge, news, or non-academic topics\n",
        "- The question asks for information beyond what academic papers would contain\n",
        "\n",
        "Consider the conversation history for context.\n",
        "\n",
        "Question: {question}\n",
        "Conversation History: {conversation_history}\n",
        "\n",
        "Respond with ONLY ONE of these two options:\n",
        "\"arxiv\" - if the question should be answered using research papers\n",
        "\"web\" - if the question requires web search\n",
        "\n",
        "Your decision should be a single word only (either \"arxiv\" or \"web\"). Do not include any explanation, reasoning, or additional text in your response.\n",
        "\"\"\")\n",
        "\n",
        "def router_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Determines whether to use ArXiv papers or web search based on the question.\n",
        "\n",
        "    Args:\n",
        "        state: Current state containing the question and conversation history\n",
        "\n",
        "    Returns:\n",
        "        Dict indicating which path to take next\n",
        "    \"\"\"\n",
        "    # Use a lighter model for routing decisions\n",
        "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "    # Create a chain that outputs just the decision text\n",
        "    chain = router_prompt | llm\n",
        "\n",
        "    # Invoke the chain with our question and history\n",
        "    # Get the content of the AIMessage object instead of directly calling strip()\n",
        "    decision = chain.invoke({\n",
        "        \"question\": state[\"question\"],\n",
        "        \"conversation_history\": state[\"conversation_history\"]\n",
        "    }).content.strip().lower()\n",
        "\n",
        "    print(f\"Router decision: {decision}\")\n",
        "\n",
        "    # Return the next node to be called based on the decision\n",
        "    if \"web\" in decision:\n",
        "        return {\"next\": \"web_search\"}\n",
        "    else:\n",
        "        return {\"next\": \"arxiv_retrieval\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D_WGWpKhz4D"
      },
      "source": [
        "# ArXiv Processor Documentation\n",
        "\n",
        "## Overview\n",
        "The `ArXivProcessor` class is designed to handle processing ArXiv PDFs for retrieval-augmented generation (RAG) systems. It implements document-aware chunking strategies specifically optimized for scientific papers.\n",
        "\n",
        "## Key Features\n",
        "- **Two-step chunking strategy**:\n",
        " 1. Markdown header splitting to preserve document structure\n",
        " 2. Recursive character splitting for handling longer sections effectively\n",
        "- **Confidence-based retrieval** with threshold filtering\n",
        "- **Metadata preservation** from original PDFs\n",
        "\n",
        "## Class Structure\n",
        "\n",
        "### Constructor: `__init__()`\n",
        "Initializes the processor with specialized document chunking strategies:\n",
        "- `MarkdownHeaderTextSplitter` to maintain document section structure\n",
        "- `RecursiveCharacterTextSplitter` for detailed content subdivision\n",
        "\n",
        "### Methods\n",
        "\n",
        "#### `load_and_process(pdf_urls: List[str])`\n",
        "Processes ArXiv PDFs with document-aware chunking:\n",
        "- Loads PDFs from provided URLs\n",
        "- Converts content to markdown-style text with headers\n",
        "- Applies two-stage chunking process\n",
        "- Creates a vector store with OpenAI embeddings\n",
        "\n",
        "#### `retrieve(question: str, confidence_threshold: float = 0.75, k: int = 5)`\n",
        "Retrieves relevant chunks with confidence scoring:\n",
        "- Performs similarity search based on user query\n",
        "- Filters results by confidence threshold\n",
        "- Returns only high-relevance document chunks\n",
        "\n",
        "## Implementation Example\n",
        "The documented code includes a sample implementation that loads and processes two ArXiv papers:\n",
        "- Quantum computing paper: https://arxiv.org/pdf/2305.10343.pdf\n",
        "- LLM research paper: https://arxiv.org/pdf/2303.04137.pdf\n",
        "\n",
        "## Dependencies\n",
        "- `PyPDFLoader` for PDF handling\n",
        "- `MarkdownHeaderTextSplitter` and `RecursiveCharacterTextSplitter` for content chunking\n",
        "- `OpenAIEmbeddings` for vector embeddings\n",
        "- `Chroma` for vector storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeTPjOKgdom7",
        "outputId": "e30650b4-a656-499f-9250-a948c6a5e5e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing ArXiv processor with sample papers...\n",
            "Loading PDF from https://arxiv.org/pdf/2305.10343.pdf\n",
            "Loading PDF from https://arxiv.org/pdf/2303.04137.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf.generic._base:could not convert string to float: b'0.00-28645087' : FloatObject (b'0.00-28645087') invalid; use 0.0 instead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 171 chunks from 2 PDFs\n",
            "ArXiv processor initialized!\n"
          ]
        }
      ],
      "source": [
        "class ArXivProcessor:\n",
        "    \"\"\"\n",
        "    Handles processing ArXiv PDFs for retrieval-augmented generation.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the processor with document-aware chunking strategies.\n",
        "\n",
        "        The chunking strategy uses a two-step approach:\n",
        "        1. Markdown header splitting preserves document structure and headers\n",
        "        2. Recursive character splitting handles longer sections effectively\n",
        "        \"\"\"\n",
        "        # Header splitter preserves section structure in scientific papers\n",
        "        self.header_splitter = MarkdownHeaderTextSplitter(\n",
        "            headers_to_split_on=[\n",
        "                (\"#\", \"Section\"),           # Main sections\n",
        "                (\"##\", \"Subsection\"),       # Subsections\n",
        "                (\"###\", \"Subsubsection\")    # Sub-subsections\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Recursive splitter handles nested hierarchies and technical content\n",
        "        # - Chunk size of 1000 balances context vs specificity\n",
        "        # - Overlap of 200 ensures continuity between chunks\n",
        "        # - Separators prioritize natural breaks in scientific text\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
        "        )\n",
        "\n",
        "        # Will be initialized when documents are loaded\n",
        "        self.vector_store = None\n",
        "\n",
        "    def load_and_process(self, pdf_urls: List[str]):\n",
        "        \"\"\"\n",
        "        Process ArXiv PDFs with document-aware chunking\n",
        "\n",
        "        Args:\n",
        "            pdf_urls: List of URLs to ArXiv PDFs\n",
        "        \"\"\"\n",
        "        all_chunks = []\n",
        "\n",
        "        # Process each PDF\n",
        "        for url in pdf_urls:\n",
        "            print(boxen(f\"Loading PDF from {url}\", title=\">>> PDF Loading\", color=\"blue\", padding=1))\n",
        "            loader = PyPDFLoader(url)\n",
        "            pages = loader.load()\n",
        "\n",
        "            # Process each page\n",
        "            for page in pages:\n",
        "                # Convert PDF content to markdown-style text with headers\n",
        "                page_text = f\"# {page.metadata['source']}\\n## Page {page.metadata['page']}\\n{page.page_content}\"\n",
        "\n",
        "                # First split by headers to maintain document structure\n",
        "                header_chunks = self.header_splitter.split_text(page_text)\n",
        "\n",
        "                # Then split large sections into smaller chunks\n",
        "                small_chunks = self.text_splitter.split_documents(header_chunks)\n",
        "\n",
        "                # Add to our collection\n",
        "                all_chunks.extend(small_chunks)\n",
        "\n",
        "        print(boxen(f\"Created {len(all_chunks)} chunks from {len(pdf_urls)} PDFs\", title=\">>> Processing Complete\", color=\"green\", padding=1))\n",
        "\n",
        "        # Create vector store with OpenAI embeddings\n",
        "        self.vector_store = Chroma.from_documents(\n",
        "            documents=all_chunks,\n",
        "            embedding=OpenAIEmbeddings(),\n",
        "            persist_directory=\"./arxiv_db\"\n",
        "        )\n",
        "\n",
        "    def retrieve(self, question: str, confidence_threshold: float = 0.75, k: int = 5):\n",
        "        \"\"\"\n",
        "        Retrieve relevant chunks with confidence scoring\n",
        "\n",
        "        Args:\n",
        "            question: User question to find relevant information for\n",
        "            confidence_threshold: Minimum relevance score (0-1) to include a result\n",
        "            k: Maximum number of results to return\n",
        "\n",
        "        Returns:\n",
        "            List of relevant document chunks that meet the threshold\n",
        "        \"\"\"\n",
        "        if not self.vector_store:\n",
        "            raise ValueError(\"No ArXiv documents loaded. Run load_and_process first.\")\n",
        "\n",
        "        # Perform similarity search with relevance scores\n",
        "        results = self.vector_store.similarity_search_with_relevance_scores(\n",
        "            question, k=k\n",
        "        )\n",
        "\n",
        "        # Filter by confidence threshold\n",
        "        filtered_results = [doc for doc, score in results if score >= confidence_threshold]\n",
        "\n",
        "        print(boxen(f\"Found {len(filtered_results)} relevant chunks above threshold {confidence_threshold}\", title=\">>> Retrieval Results\", color=\"yellow\", padding=1))\n",
        "\n",
        "        return filtered_results\n",
        "\n",
        "# Load sample ArXiv PDFs\n",
        "print(boxen(\"Initializing ArXiv processor with sample papers...\", title=\">>> Initialization\", color=\"cyan\", padding=1))\n",
        "arxiv_processor = ArXivProcessor()\n",
        "arxiv_processor.load_and_process([\n",
        "    \"https://arxiv.org/pdf/2305.10343.pdf\",  # Quantum computing paper\n",
        "    \"https://arxiv.org/pdf/2303.04137.pdf\"   # LLM research paper\n",
        "])\n",
        "print(boxen(\"ArXiv processor initialized!\", title=\">>> Status\", color=\"green\", padding=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ArXiv Retrieval Node Documentation\n",
        "\n",
        "## Overview\n",
        "The `arxiv_retrieval_node` function serves as a retrieval component in an agent-based system, fetching relevant scientific information from ArXiv papers based on user queries.\n",
        "\n",
        "## Function Signature\n",
        "`arxiv_retrieval_node(state: AgentState) -> dict`\n",
        "\n",
        "## Parameters\n",
        "- `state`: An AgentState object containing the current conversation state, including:\n",
        " - `question`: The user's query to search for in ArXiv papers\n",
        "\n",
        "## Functionality\n",
        "The function:\n",
        "1. Extracts the user's question from the input state\n",
        "2. Calls the `arxiv_processor.retrieve()` method to find relevant document chunks\n",
        "3. Uses a reduced confidence threshold (0.5) compared to the default (0.75) to improve recall\n",
        "4. Returns the retrieved documents for further processing\n",
        "\n",
        "## Return Value\n",
        "Returns a dictionary with:\n",
        "- `arxiv_results`: A list of document chunks from ArXiv papers relevant to the user's question\n",
        "\n",
        "## Integration Notes\n",
        "- This function is designed to be used as a node in an agent workflow\n",
        "- The reduced confidence threshold ensures more potential matches are returned, prioritizing recall over precision\n",
        "- The retrieved documents can be used by subsequent nodes for answering the user's question"
      ],
      "metadata": {
        "id": "OeaD77BupxMv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7QI0RXSbdvCv"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "def arxiv_retrieval_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Retrieves relevant information from ArXiv papers based on the question.\n",
        "\n",
        "    Args:\n",
        "        state: Current state containing the question\n",
        "\n",
        "    Returns:\n",
        "        Updated state with arxiv_results\n",
        "    \"\"\"\n",
        "    # Retrieve relevant documents from ArXiv\n",
        "    relevant_docs = arxiv_processor.retrieve(\n",
        "        question=state[\"question\"],\n",
        "        confidence_threshold=0.5  # Adjusted threshold for better recall\n",
        "    )\n",
        "\n",
        "    # Check if we found enough relevant content\n",
        "    return {\"arxiv_results\": relevant_docs}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SqO1XREiD7z"
      },
      "source": [
        "## 5. Web Search Node Implementation\n",
        "\n",
        "The **Web Search Node** uses the **Tavily API** to search the web when **ArXiv papers** don't have the answer.\n",
        "\n",
        "- **Optimizes** the search query\n",
        "- **Filters and processes** results\n",
        "- **Ensures** proper attribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "15KqqPGedzln"
      },
      "outputs": [],
      "source": [
        "class WebSearcher:\n",
        "    \"\"\"\n",
        "    Handles web search functionality using the Tavily API.\n",
        "    \"\"\"\n",
        "    def __init__(self, api_key: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Initialize with Tavily API client.\n",
        "\n",
        "        Args:\n",
        "            api_key: Tavily API key (uses environment variable if None)\n",
        "        \"\"\"\n",
        "        if api_key:\n",
        "            self.client = TavilyClient(api_key=api_key)\n",
        "        else:\n",
        "            self.client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
        "\n",
        "    def search(self, query: str, max_results: int = 5, include_domains: Optional[List[str]] = None):\n",
        "        \"\"\"\n",
        "        Search the web with query optimization.\n",
        "\n",
        "        Args:\n",
        "            query: User question\n",
        "            max_results: Maximum number of results to return\n",
        "            include_domains: Optional list of domains to prioritize\n",
        "\n",
        "        Returns:\n",
        "            List of search results with content and URL\n",
        "        \"\"\"\n",
        "        # Optimize the query for academic research\n",
        "        # Biases towards academic and scientific sources\n",
        "        domain_filter = \"\"\n",
        "        if include_domains:\n",
        "            domain_filter = \" OR \".join([f\"site:{domain}\" for domain in include_domains])\n",
        "            domain_filter = f\"({domain_filter}) \"\n",
        "\n",
        "        optimized_query = f\"{domain_filter}{query}\"\n",
        "\n",
        "        print(boxen(f\"Searching with query: {optimized_query}\", title=\">>> Web Search Query\", color=\"blue\", padding=1))\n",
        "\n",
        "        # Execute search through Tavily\n",
        "        response = self.client.search(\n",
        "            query=optimized_query,\n",
        "            search_depth=\"advanced\",  # More comprehensive search\n",
        "            include_answer=True,      # Get an AI-generated answer too\n",
        "            max_results=max_results\n",
        "        )\n",
        "\n",
        "        # Extract relevant information from results\n",
        "        results = [\n",
        "            {\n",
        "                \"content\": r[\"content\"],\n",
        "                \"url\": r[\"url\"],\n",
        "                \"title\": r.get(\"title\", \"Unknown Title\")\n",
        "            }\n",
        "            for r in response[\"results\"]\n",
        "        ]\n",
        "\n",
        "        print(boxen(f\"Found {len(results)} web results\", title=\">>> Search Results\", color=\"yellow\", padding=1))\n",
        "\n",
        "        return results\n",
        "\n",
        "# Initialize web searcher\n",
        "web_searcher = WebSearcher()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_WsFmnmpd3CI"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "def web_search_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Searches the web for information using the Tavily API.\n",
        "\n",
        "    Args:\n",
        "        state: Current state containing the question\n",
        "\n",
        "    Returns:\n",
        "        Updated state with web_results\n",
        "    \"\"\"\n",
        "    # Include academic domains to improve search quality\n",
        "    academic_domains = [\"arxiv.org\", \"scholar.google.com\", \"researchgate.net\", \"edu\"]\n",
        "\n",
        "    # Get search results\n",
        "    results = web_searcher.search(\n",
        "        query=state[\"question\"],\n",
        "        max_results=5,\n",
        "        include_domains=academic_domains\n",
        "    )\n",
        "\n",
        "    return {\"web_results\": results}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYoo-tCciKHn"
      },
      "source": [
        "# Function Documentation: `synthesize_answer_node`\n",
        "\n",
        "## Overview\n",
        "The `synthesize_answer_node` function is a key component in a LangChain-based conversational agent. It is responsible for generating a comprehensive answer based on either scientific research papers (from ArXiv) or web search results (via Tavily). The generated response is contextual, well-structured, and strictly grounded in the retrieved data.\n",
        "\n",
        "---\n",
        "\n",
        "## Purpose\n",
        "To synthesize a high-quality, structured, and citation-backed answer from the information retrieved during the conversational flow — either from ArXiv research papers or real-time web search results.\n",
        "\n",
        "---\n",
        "\n",
        "## Inputs\n",
        "\n",
        "- **state (AgentState)**:  \n",
        "  A dictionary representing the current state of the agent, which includes:\n",
        "  - `question`: The user's query.\n",
        "  - `arxiv_results`: A list of research paper excerpts (if available).\n",
        "  - `web_results`: A list of web search results (used when no ArXiv data is present).\n",
        "  - `conversation_history`: Context from previous exchanges to maintain continuity.\n",
        "\n",
        "---\n",
        "\n",
        "## Logic Flow\n",
        "\n",
        "1. **Source Determination**:  \n",
        "   The function first checks whether ArXiv results are available. If so, it uses them; otherwise, it falls back to web search results.\n",
        "\n",
        "2. **Prompt Construction**:  \n",
        "   A custom `prompt_template` is built depending on the data source. Each template includes:\n",
        "   - The original question.\n",
        "   - Retrieved content (formatted accordingly).\n",
        "   - Prior conversation context.\n",
        "   - Explicit instructions to ensure grounded, factual, and well-structured responses.\n",
        "\n",
        "3. **Model Invocation**:  \n",
        "   - Uses `ChatOpenAI` (specifically `gpt-4-turbo`) for advanced reasoning and response generation.\n",
        "   - Combines the prompt and model into a LangChain chain using `ChatPromptTemplate` and `StrOutputParser`.\n",
        "\n",
        "4. **Response Handling**:  \n",
        "   - If web results were used, the function appends a list of source URLs at the end of the response.\n",
        "   - If ArXiv sources were used, inline citations in the format `(Author et al., Page X)` are expected.\n",
        "\n",
        "---\n",
        "\n",
        "## Output\n",
        "\n",
        "- Returns a dictionary with a single key:  \n",
        "  - `answer`: A fully formatted, cited response derived from either research papers or search results.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Characteristics\n",
        "\n",
        "- **Grounded Output**: The model is instructed not to hallucinate or invent facts.\n",
        "- **Citations Included**: Adds credibility and traceability via inline citations or URL references.\n",
        "- **Context-Aware**: Maintains conversation context to provide coherent multi-turn interactions.\n",
        "- **Readable Format**: Uses markdown elements such as headers, bullet points, and bold text for readability.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "3h2iGpE9d8VM"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "def synthesize_answer_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Synthesizes a comprehensive answer from retrieved information.\n",
        "\n",
        "    Args:\n",
        "        state: Current state containing question and retrieved information\n",
        "\n",
        "    Returns:\n",
        "        Updated state with answer\n",
        "    \"\"\"\n",
        "    # Determine which source to use for synthesis\n",
        "    if state[\"arxiv_results\"] and len(state[\"arxiv_results\"]) > 0:\n",
        "        # Using ArXiv research papers\n",
        "        sources = \"\\n\\n\".join([\n",
        "            f\"--- Document: {d.metadata.get('source', 'Unknown')} (Page {d.metadata.get('page', 'Unknown')}) ---\\n{d.page_content}\"\n",
        "            for d in state[\"arxiv_results\"]\n",
        "        ])\n",
        "\n",
        "        prompt_template =\"\"\"\n",
        "        You are a knowledgeable research assistant specializing in mathematical theory and scientific literature analysis.\n",
        "\n",
        "        Your goal is to generate clean, formatted responses to user questions based solely on the provided ArXiv sources.\n",
        "\n",
        "        ---\n",
        "\n",
        "        Question:\n",
        "        {question}\n",
        "\n",
        "        Relevant Extracts from ArXiv Papers:\n",
        "        {sources}\n",
        "\n",
        "        Conversation History:\n",
        "        {conversation_history}\n",
        "\n",
        "        ---\n",
        "\n",
        "        Instructions for Synthesizing the Answer:\n",
        "\n",
        "        1. Read the extracts thoroughly and understand the concepts.\n",
        "        2. Answer the question comprehensively using only the provided context.\n",
        "        3. Organize the response into the following markdown sections (if applicable):\n",
        "          - Summary\n",
        "          - Key Concepts\n",
        "          - Theoretical Results\n",
        "          - Implications / Applications\n",
        "        4. Cite from the paper in the format: (Author et al., Page X). If page number is unknown, write: (Author et al.).\n",
        "        6. Avoid repetition, excessive formal tone, or generic commentary. Be clear and concise.**\n",
        "        7. If the provided text lacks enough detail to answer, state it clearly and suggest what additional info is needed.\n",
        "\n",
        "        ---\n",
        "\n",
        "        Now, write a well-structured, markdown-formatted answer to the question and it should be in a readable format as well.\n",
        "        \"\"\"\n",
        "    else:\n",
        "        # Using web search results\n",
        "        sources = \"\\n\\n\".join([\n",
        "            f\"--- Source {i+1}: {res['title']} ---\\n{res['content']}\"\n",
        "            for i, res in enumerate(state[\"web_results\"] or [])\n",
        "        ])\n",
        "\n",
        "        prompt_template = \"\"\"\n",
        "        You are a knowledgeable research assistant providing accurate information based on web search results.\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Here are relevant web search results:\n",
        "        {sources}\n",
        "\n",
        "        Conversation History:\n",
        "        {conversation_history}\n",
        "\n",
        "        Instructions:\n",
        "        1. Synthesize a comprehensive answer using ONLY the information provided above.\n",
        "        2. Cite sources using [1], [2], etc. corresponding to the source numbers above.\n",
        "        3. If the search results don't contain sufficient information, acknowledge the limitations.\n",
        "        4. DO NOT make up information not present in the sources.\n",
        "        5. Include only facts supported by the sources.\n",
        "\n",
        "        Your answer:\n",
        "        \"\"\"\n",
        "\n",
        "    # Create the prompt\n",
        "    synthesis_prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "\n",
        "    # Use a more capable model for synthesis\n",
        "    llm = ChatOpenAI(model=\"gpt-4-turbo\")\n",
        "    chain = synthesis_prompt | llm | StrOutputParser()\n",
        "\n",
        "    # Generate the answer\n",
        "    response = chain.invoke({\n",
        "        \"question\": state[\"question\"],\n",
        "        \"sources\": sources,\n",
        "        \"conversation_history\": state[\"conversation_history\"]\n",
        "    })\n",
        "\n",
        "    # Add source citations for web results\n",
        "    if state.get(\"web_results\") and not state.get(\"arxiv_results\"):\n",
        "        answer_content = response\n",
        "\n",
        "        # Add URL references at the end\n",
        "        url_citations = \"\\n\\nSources:\\n\" + \"\\n\".join([\n",
        "            f\"[{i+1}] {res['url']}\"\n",
        "            for i, res in enumerate(state[\"web_results\"] or [])\n",
        "        ])\n",
        "\n",
        "        answer_content += url_citations\n",
        "    else:\n",
        "        answer_content = response\n",
        "\n",
        "    return {\"answer\": answer_content}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePSZSMQBiaMC"
      },
      "source": [
        "## 7. Conversation Memory Node\n",
        "\n",
        "This node **manages conversation history** to provide context for **multi-turn interactions**.\n",
        "\n",
        "- **Stores** previous Q&A\n",
        "- **Updates** the state with the current interaction\n",
        "- **Maintains** a sliding window of relevant history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Pgefbaa_eAfZ"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "# Initialize conversation memory\n",
        "memory = ConversationBufferMemory(return_messages=False, output_key=\"answer\", input_key=\"question\")\n",
        "\n",
        "def update_memory_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Updates the conversation memory with the current Q&A pair.\n",
        "\n",
        "    Args:\n",
        "        state: Current state with question and answer\n",
        "\n",
        "    Returns:\n",
        "        Updated state with new conversation_history\n",
        "    \"\"\"\n",
        "    # Save the current interaction to memory\n",
        "    memory.save_context(\n",
        "        {\"question\": state[\"question\"]},\n",
        "        {\"answer\": state[\"answer\"]}\n",
        "    )\n",
        "\n",
        "    # Get the updated conversation history\n",
        "    updated_history = memory.load_memory_variables({})[\"history\"]\n",
        "\n",
        "    # Return the updated state\n",
        "    return {\"conversation_history\": updated_history}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxKEeu64iglN"
      },
      "source": [
        "# Workflow State Graph Setup\n",
        "\n",
        "## Overview\n",
        "This section sets up the **LangGraph state machine** for managing the conversational agent’s workflow. It defines how user queries are processed step-by-step using modular nodes.\n",
        "\n",
        "---\n",
        "\n",
        "## Purpose\n",
        "To create a graph-based control flow that determines how the agent processes input, performs retrieval, synthesizes responses, updates memory, and eventually ends the workflow.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Components\n",
        "\n",
        "### 1. **Workflow Initialization**\n",
        "- A new `StateGraph` is initialized with the `AgentState` type, defining the structure of the workflow.\n",
        "\n",
        "### 2. **Node Definitions**\n",
        "The graph is composed of several functional nodes, each responsible for a specific task:\n",
        "- **router**: Determines whether to fetch data from the web or ArXiv.\n",
        "- **arxiv_retrieval**: Retrieves relevant research papers from ArXiv.\n",
        "- **web_search**: Retrieves web results via Tavily.\n",
        "- **synthesize**: Synthesizes a final answer from the retrieved information.\n",
        "- **update_memory**: Stores the interaction context for future turns.\n",
        "\n",
        "### 3. **Entry Point**\n",
        "- The `router` node is set as the initial entry point for the graph, meaning every workflow starts with routing logic.\n",
        "\n",
        "### 4. **Conditional Routing**\n",
        "- A conditional edge is established from `router` based on the `\"next\"` field in the state:\n",
        "  - If `\"next\"` is `\"web_search\"`, it routes to the `web_search` node.\n",
        "  - If `\"next\"` is `\"arxiv_retrieval\"`, it routes to the `arxiv_retrieval` node.\n",
        "\n",
        "### 5. **Workflow Sequence**\n",
        "The following fixed transitions define the remainder of the workflow:\n",
        "- From either `web_search` or `arxiv_retrieval` → go to `synthesize`\n",
        "- From `synthesize` → go to `update_memory`\n",
        "- From `update_memory` → reach `END` (completion of the flow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "BRYbjFhHeEZt"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "# Create the workflow state graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Add all nodes to the graph\n",
        "workflow.add_node(\"router\", router_node)\n",
        "workflow.add_node(\"arxiv_retrieval\", arxiv_retrieval_node)\n",
        "workflow.add_node(\"web_search\", web_search_node)\n",
        "workflow.add_node(\"synthesize\", synthesize_answer_node)\n",
        "workflow.add_node(\"update_memory\", update_memory_node)\n",
        "\n",
        "# Set entry point\n",
        "workflow.set_entry_point(\"router\")\n",
        "\n",
        "# Define conditional edges from router\n",
        "workflow.add_conditional_edges(\n",
        "    \"router\",\n",
        "    lambda state: state[\"next\"],\n",
        "    {\n",
        "        \"web_search\": \"web_search\",\n",
        "        \"arxiv_retrieval\": \"arxiv_retrieval\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Define rest of the edges\n",
        "workflow.add_edge(\"arxiv_retrieval\", \"synthesize\")\n",
        "workflow.add_edge(\"web_search\", \"synthesize\")\n",
        "workflow.add_edge(\"synthesize\", \"update_memory\")\n",
        "workflow.add_edge(\"update_memory\", END)\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m4TP3xSioHc"
      },
      "source": [
        "## 9. Testing the System\n",
        "\n",
        "Let's **test our system** with different types of questions:\n",
        "\n",
        "- **Questions answerable** from ArXiv papers\n",
        "- **Questions requiring** web search\n",
        "- **Follow-up questions** to test memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "gFoxsXLWeHSv"
      },
      "outputs": [],
      "source": [
        "\n",
        "def ask(question: str):\n",
        "    \"\"\"\n",
        "    Ask a question to the agentic RAG system.\n",
        "\n",
        "    Args:\n",
        "        question: User's question\n",
        "\n",
        "    Returns:\n",
        "        The system's answer\n",
        "    \"\"\"\n",
        "    print(boxen(f\"Question: {question}\", title=\">>> User Question\", color=\"cyan\", padding=1))\n",
        "\n",
        "    # Initialize the state\n",
        "    initial_state = {\n",
        "        \"question\": question,\n",
        "        \"arxiv_results\": None,\n",
        "        \"web_results\": None,\n",
        "        \"answer\": \"\",\n",
        "        \"conversation_history\": memory.load_memory_variables({}).get(\"history\", \"\")\n",
        "    }\n",
        "\n",
        "    # Invoke the workflow\n",
        "    result = app.invoke(initial_state)\n",
        "\n",
        "    # Print the answer using boxen\n",
        "    print(boxen(result[\"answer\"], title=\">>> Answer\", color=\"green\", padding=1))\n",
        "\n",
        "    return result[\"answer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b0Ps7Nw1exMl",
        "outputId": "fdf4e290-53f2-42cb-ad61-5d797594e426"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m╭─\u001b[0m\u001b[36m >>> User Question \u001b[0m\u001b[36m──────────────────────────────────────────────────────────────\u001b[0m\u001b[36m─╮\u001b[0m                              \n",
            "\u001b[36m│\u001b[0m                                                                                   \u001b[36m│\u001b[0m                              \n",
            "\u001b[36m│\u001b[0m   Question: Explain the surface code implementation in quantum error correction   \u001b[36m│\u001b[0m                              \n",
            "\u001b[36m│\u001b[0m                                                                                   \u001b[36m│\u001b[0m                              \n",
            "\u001b[36m╰───────────────────────────────────────────────────────────────────────────────────╯\u001b[0m                              \n",
            "\n",
            "Router decision: arxiv\n",
            "Found 5 relevant chunks above threshold 0.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m╭─\u001b[0m\u001b[32m >>> Answer \u001b[0m\u001b[32m───────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   ## Summary                                                                                                    \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   The provided extracts do not contain detailed information on the implementation of the surface code in        \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   quantum error correction. The extracts mostly focus on mathematical theories, realizability of point          \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   processes, and truncated moment problems which are not directly related to quantum error correction or        \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   surface codes.                                                                                                \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   ## Key Concepts                                                                                               \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   Given the absence of direct information on the surface code, let's briefly touch on the fundamental           \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   concepts typically associated with quantum error correction:                                                  \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   - **Quantum error correction (QEC)**: A method used in quantum computing to protect quantum information       \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   from errors due to decoherence and other quantum noise.                                                       \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   - **Surface codes**: A class of error-correcting codes that use a two-dimensional lattice of qubits. They     \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   are robust against a reasonable error rate and can be implemented with local interactions on a quantum        \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   computer.                                                                                                     \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   ## Theoretical Results                                                                                        \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   As the extracts provided do not discuss surface codes or their implementation, no specific theoretical        \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   results regarding surface code can be derived from the current documents.                                     \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   ## Implications / Applications                                                                                \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   The general implications of surface codes in quantum computing include:                                       \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   - Enhancing the reliability of quantum computations by correcting errors that arise during quantum            \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   operations.                                                                                                   \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   - Enabling scalable quantum computation by employing a structure that is conducive to practical qubit         \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   arrangement and error management.                                                                             \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   ## Limitations                                                                                                \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   The primary limitation in this response is the lack of direct references or explanations in the provided      \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   documents about the surface code or its specifics concerning quantum error correction. The documents focus    \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   on other mathematical and physical theories that do not overlap with the query.                               \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   For a comprehensive understanding of surface code implementation in quantum error correction, examination     \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   of specialized literature in quantum information theory and quantum computing would be necessary, which       \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   goes beyond the scope of the provided extracts.                                                               \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"## Summary\\n\\nThe provided extracts do not contain detailed information on the implementation of the surface code in quantum error correction. The extracts mostly focus on mathematical theories, realizability of point processes, and truncated moment problems which are not directly related to quantum error correction or surface codes.\\n\\n## Key Concepts\\n\\nGiven the absence of direct information on the surface code, let's briefly touch on the fundamental concepts typically associated with quantum error correction:\\n\\n- **Quantum error correction (QEC)**: A method used in quantum computing to protect quantum information from errors due to decoherence and other quantum noise.\\n\\n- **Surface codes**: A class of error-correcting codes that use a two-dimensional lattice of qubits. They are robust against a reasonable error rate and can be implemented with local interactions on a quantum computer.\\n\\n## Theoretical Results\\n\\nAs the extracts provided do not discuss surface codes or their implementation, no specific theoretical results regarding surface code can be derived from the current documents.\\n\\n## Implications / Applications\\n\\nThe general implications of surface codes in quantum computing include:\\n\\n- Enhancing the reliability of quantum computations by correcting errors that arise during quantum operations.\\n- Enabling scalable quantum computation by employing a structure that is conducive to practical qubit arrangement and error management.\\n\\n## Limitations\\n\\nThe primary limitation in this response is the lack of direct references or explanations in the provided documents about the surface code or its specifics concerning quantum error correction. The documents focus on other mathematical and physical theories that do not overlap with the query.\\n\\nFor a comprehensive understanding of surface code implementation in quantum error correction, examination of specialized literature in quantum information theory and quantum computing would be necessary, which goes beyond the scope of the provided extracts.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# Test with a question about quantum computing (should use ArXiv)\n",
        "ask(\"Explain the surface code implementation in quantum error correction\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7keGgbrWgqy9",
        "outputId": "ee3ba12e-61e8-4673-ce0d-f106d125d52b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m╭─\u001b[0m\u001b[36m >>> User Question \u001b[0m\u001b[36m────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[36m─╮\u001b[0m\n",
            "\u001b[36m│\u001b[0m                                                                                                                 \u001b[36m│\u001b[0m\n",
            "\u001b[36m│\u001b[0m   Question: What are the necessary and sufficient conditions for the existence of a representing measure in     \u001b[36m│\u001b[0m\n",
            "\u001b[36m│\u001b[0m   the realizability problem?                                                                                    \u001b[36m│\u001b[0m\n",
            "\u001b[36m│\u001b[0m                                                                                                                 \u001b[36m│\u001b[0m\n",
            "\u001b[36m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\n",
            "Router decision: arxiv\n",
            "Found 5 relevant chunks above threshold 0.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m╭─\u001b[0m\u001b[32m >>> Answer \u001b[0m\u001b[32m───────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   ### Summary                                                                                                   \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   The necessary and sufficient conditions for the existence of a representing measure in the realizability      \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   problem are deeply rooted in the analysis of the infinite-dimensional truncated moment problem. This          \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   approach was internationally recognized through the generalization of the Riesz-Markov theorem and further    \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   underpinned by recent advancements in the treatment of linear functionals on unital commutative algebras.     \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   ### Key Concepts                                                                                              \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   - **Realizability Problem**: A complex system can be modeled as an infinite-dimensional moment problem,       \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   specifically a truncated K−moment problem, where K represents the space of all configurations of the system   \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   components.                                                                                                   \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   - **Representing Measure**: It concerns whether a given sequence (correlation functions) can represent a      \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   measure (i.e., if there exists a measure that has these sequences as its moments).                            \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   ### Theoretical Results                                                                                       \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   1. **General Conditions:** T. Kuna, J. L. Lebowitz, and E. R. Speer have established necessary and            \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   sufficient conditions for the realizability of point processes based on the infinite-dimensional truncated    \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   moment framework (Kuna et al., 2011).                                                                         \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   2. **Advancements using Unital Commutative Algebras:** The recent advancements cited in the paper focus on    \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   the truncated moment problem and how these can be applied to provide alternative proofs and extensions of     \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   the main results in the issues of representing measures related to the realizability problem. Profound        \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   reliance lies on the insight gained from truncated moment problems, where reformulation has allowed broader   \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   explanatory and proof mechanisms under Haviland-type conditions (R.E. Curto and M. Infusino).                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   ### Implications / Applications                                                                               \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   The theoretical underpinnings form a pivotal role in various applications ranging from statistical            \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   mechanics to quantum many-body theory. The ability to confirm whether a sequence of moments corresponds to    \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   a representing measure can deeply influence the understanding and construction of complex systems,            \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   especially in systems where configurations and component interactions are critical. This impacts how          \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   theoretical formulations translate into practical models in physics and probability.                          \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   ### Conclusion                                                                                                \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   In conclusion, the capacity to align the realizability problem with the theoretical constructs of             \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   infinite-dimensional truncated moment problems has provided a robust mathematical foundation for              \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   identifying necessary and sufficient conditions for the existence of a representing measure. This             \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   development encapsulates significant strides in both theoretical and applied aspects of mathematical and      \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   physical sciences.                                                                                            \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   **References:**                                                                                               \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   (Kuna, Lebowitz, and Speer, 2011);                                                                            \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   (R.E. Curto and M. Infusino).                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   This structure provides clarity on the interwoven complexity and the path forward in leveraging these         \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   results in both theory development and practical application contexts. If further details on specific         \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   applications or proofs are required, consulting detailed sections or papers specified would be beneficial.    \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'### Summary\\nThe necessary and sufficient conditions for the existence of a representing measure in the realizability problem are deeply rooted in the analysis of the infinite-dimensional truncated moment problem. This approach was internationally recognized through the generalization of the Riesz-Markov theorem and further underpinned by recent advancements in the treatment of linear functionals on unital commutative algebras.\\n\\n### Key Concepts\\n- **Realizability Problem**: A complex system can be modeled as an infinite-dimensional moment problem, specifically a truncated K−moment problem, where K represents the space of all configurations of the system components.\\n- **Representing Measure**: It concerns whether a given sequence (correlation functions) can represent a measure (i.e., if there exists a measure that has these sequences as its moments).\\n\\n### Theoretical Results\\n1. **General Conditions:** T. Kuna, J. L. Lebowitz, and E. R. Speer have established necessary and sufficient conditions for the realizability of point processes based on the infinite-dimensional truncated moment framework (Kuna et al., 2011).\\n2. **Advancements using Unital Commutative Algebras:** The recent advancements cited in the paper focus on the truncated moment problem and how these can be applied to provide alternative proofs and extensions of the main results in the issues of representing measures related to the realizability problem. Profound reliance lies on the insight gained from truncated moment problems, where reformulation has allowed broader explanatory and proof mechanisms under Haviland-type conditions (R.E. Curto and M. Infusino).\\n\\n### Implications / Applications\\nThe theoretical underpinnings form a pivotal role in various applications ranging from statistical mechanics to quantum many-body theory. The ability to confirm whether a sequence of moments corresponds to a representing measure can deeply influence the understanding and construction of complex systems, especially in systems where configurations and component interactions are critical. This impacts how theoretical formulations translate into practical models in physics and probability.\\n\\n### Conclusion\\nIn conclusion, the capacity to align the realizability problem with the theoretical constructs of infinite-dimensional truncated moment problems has provided a robust mathematical foundation for identifying necessary and sufficient conditions for the existence of a representing measure. This development encapsulates significant strides in both theoretical and applied aspects of mathematical and physical sciences.\\n\\n**References:** \\n(Kuna, Lebowitz, and Speer, 2011); \\n(R.E. Curto and M. Infusino).\\n\\nThis structure provides clarity on the interwoven complexity and the path forward in leveraging these results in both theory development and practical application contexts. If further details on specific applications or proofs are required, consulting detailed sections or papers specified would be beneficial.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# Test with a question about recent developments (should use web)\n",
        "ask(\"What are the necessary and sufficient conditions for the existence of a representing measure in the realizability problem?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M_IbwltMg2l8",
        "outputId": "870c1408-585e-413b-9201-011cd4ad8a63"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m╭─\u001b[0m\u001b[36m >>> User Question \u001b[0m\u001b[36m───────────────────────────────────────────\u001b[0m\u001b[36m─╮\u001b[0m                                                 \n",
            "\u001b[36m│\u001b[0m                                                                \u001b[36m│\u001b[0m                                                 \n",
            "\u001b[36m│\u001b[0m   Question: How does this compare to topological approaches?   \u001b[36m│\u001b[0m                                                 \n",
            "\u001b[36m│\u001b[0m                                                                \u001b[36m│\u001b[0m                                                 \n",
            "\u001b[36m╰────────────────────────────────────────────────────────────────╯\u001b[0m                                                 \n",
            "\n",
            "Router decision: arxiv\n",
            "Found 5 relevant chunks above threshold 0.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m╭─\u001b[0m\u001b[32m >>> Answer \u001b[0m\u001b[32m───────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   ## Comparison to Topological Approaches                                                                       \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   Based on the provided extracts from ArXiv research papers, there is minimal direct information that relates   \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   specifically to topological approaches, and there is no explicit comparison with any topological methods      \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   like topological insulators, topological data analysis, or other uses of topology in computational fields.    \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   The documents primarily focus on advancements in robotics, machine learning, and policy improvement through   \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   diffusion models. Here’s how the findings from the extracts relate only tangentially to the concept of        \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   topology:                                                                                                     \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   ### Key Highlights from the Documents:                                                                        \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   - **Diffusion Models**: There is a heavy emphasis on the use of diffusion models in various tasks (multiple   \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   ArXiv extracts). Diffusion models are generative models that could theoretically provide a basis for new      \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   kinds of topological optimization or analysis by allowing the exploration of a configuration space through    \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   stochastic processes, though no direct link to topology is mentioned.                                         \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   - **Performance Improvements**: The Diffusion Policy is highlighted for enabling significant performance      \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   enhancements in tasks that involve multimodal actions (Extract 2). While not directly a topological           \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   approach, the notion of managing diverse possible states can tangentially relate to explorations in           \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   configuration spaces, a mainstay in topological studies.                                                      \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   - **Multimodality Handling**: Discussion surrounding the handling of multimodality in tasks (Extract 1,       \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   Extract 2) might conceptually touch upon topological concepts of navigating complex landscapes, albeit        \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   without explicit topological analysis tools or concepts mentioned.                                            \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   ### Limitations in Relation to Topological Approaches:                                                        \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   - **Absence of Explicit Mention**: There is no explicit mention of topological methods or comparisons with    \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   topological approaches, limiting the ability to make a direct comparison.                                     \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   - **Lack of Focused Discussion**: The discussion predominantly revolves around policy models, control         \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   policies in robotics, and machine learning, with no direct relevance to the mathematical or theoretical       \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   framework of topology.                                                                                        \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   ### Conclusion:                                                                                               \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   Given the available information from the provided extracts, a direct comparison to topological approaches     \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   is not feasible due to the absence of specific discussions or comparisons relevant to topological methods     \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   or theory. While there are some tangential relations to concepts that could be explored in topological        \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   contexts, such as configuration space exploration and managing complex state spaces, these are not explored   \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   in the context of topological theories or tools. For a detailed comparison or analysis, more focused          \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m   material on topology within the mentioned contexts would be necessary.                                        \u001b[32m│\u001b[0m\n",
            "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
            "\u001b[32m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'## Comparison to Topological Approaches\\n\\nBased on the provided extracts from ArXiv research papers, there is minimal direct information that relates specifically to topological approaches, and there is no explicit comparison with any topological methods like topological insulators, topological data analysis, or other uses of topology in computational fields. The documents primarily focus on advancements in robotics, machine learning, and policy improvement through diffusion models. Here’s how the findings from the extracts relate only tangentially to the concept of topology:\\n\\n### Key Highlights from the Documents:\\n- **Diffusion Models**: There is a heavy emphasis on the use of diffusion models in various tasks (multiple ArXiv extracts). Diffusion models are generative models that could theoretically provide a basis for new kinds of topological optimization or analysis by allowing the exploration of a configuration space through stochastic processes, though no direct link to topology is mentioned.\\n- **Performance Improvements**: The Diffusion Policy is highlighted for enabling significant performance enhancements in tasks that involve multimodal actions (Extract 2). While not directly a topological approach, the notion of managing diverse possible states can tangentially relate to explorations in configuration spaces, a mainstay in topological studies.\\n- **Multimodality Handling**: Discussion surrounding the handling of multimodality in tasks (Extract 1, Extract 2) might conceptually touch upon topological concepts of navigating complex landscapes, albeit without explicit topological analysis tools or concepts mentioned.\\n\\n### Limitations in Relation to Topological Approaches:\\n- **Absence of Explicit Mention**: There is no explicit mention of topological methods or comparisons with topological approaches, limiting the ability to make a direct comparison.\\n- **Lack of Focused Discussion**: The discussion predominantly revolves around policy models, control policies in robotics, and machine learning, with no direct relevance to the mathematical or theoretical framework of topology.\\n\\n### Conclusion:\\n\\nGiven the available information from the provided extracts, a direct comparison to topological approaches is not feasible due to the absence of specific discussions or comparisons relevant to topological methods or theory. While there are some tangential relations to concepts that could be explored in topological contexts, such as configuration space exploration and managing complex state spaces, these are not explored in the context of topological theories or tools. For a detailed comparison or analysis, more focused material on topology within the mentioned contexts would be necessary.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# %% [code]\n",
        "# Test with a follow-up question (tests memory)\n",
        "ask(\"How does this compare to topological approaches?\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}